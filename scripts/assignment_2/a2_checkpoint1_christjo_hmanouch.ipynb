{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0d943c",
   "metadata": {},
   "source": [
    "## <center>CSE 546: Reinforcement Learning</center>\n",
    "### <center>Prof. Alina Vereshchaka</center>\n",
    "<!-- ### <center>Fall 2022</center> -->\n",
    "\n",
    "Welcome to the Assignment 2, Part 1: Introduction to Deep Reinforcement Learning and Neural Networks! The goal of this assignment is to make you comfortable with the application of different Neural Network structures depending on how the Reinforcement Learning environment is set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0fc19e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:30:19.772012Z",
     "start_time": "2024-09-24T11:30:13.828260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from environment import WumpusWorldEnvironment\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87890afd",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc8362",
   "metadata": {},
   "source": [
    "We will be working with an implementation of the Wumpus World environment. The environment comes from the book \"Artificial Intelligence: A Modern Approach\" by Stuart J. Russell and Peter Norvig. \n",
    "\n",
    "### ENVIRONMENT DETAILS:\n",
    "\n",
    "The environment is a 6 x 6 grid world containing a total of 36 grid blocks. \n",
    "\n",
    "#### ENVIRONMENT OBJECTS:\n",
    "The environment consists of the following objects:\n",
    "\n",
    "1. **Agent** - The agent starts in the grid block at the bottom left corner whose co-ordinates are [0, 0]. The goal of our agent is to collect the Gold while avoiding the Wumpus and the pits. \n",
    "\n",
    "2. **Wumpus** - The monster which would eat the agent if they are in the same grid block.\n",
    "\n",
    "3. **Pit** - The agent must avoid falling into the pits. \n",
    "\n",
    "4. **Gold** - The agent must collect the Gold.\n",
    "\n",
    "5. **Breeze** - Breeze surrounds the Pits and warn the agent of a Pit in an adjacent grid block.\n",
    "\n",
    "6. **Stench** - Stench surrounds the Wumpus and warns the agent of the Wumpus in an adjacent grid block.\n",
    "\n",
    "#### ENVIRONMENT OBSERVATIONS:\n",
    "\n",
    "Our implementation of the environment provides you with four different types of observations:\n",
    "\n",
    "1. **Integer** - Integer in the range [0 - 35]. This represents the grid block the agent is in. E.g., if the agent is in the bottom left grid block (starting position) the observation would be 0, if the agent is in the grid block containing the Gold the observation would be 34, if the agent is in the top right grid block the observation would be 35.\n",
    "\n",
    "2. **Vector** - \n",
    "\n",
    "    **2.1.** A vector of length 2 representing the agent co-ordinates. The first entry represents the x co-ordinate and the second entry represets the y co-ordinate. E.g., if the agent is in the bottom left grid block (starting position) the observation would be [0, 0], if the agent is in the grid block containing the Gold the observation would be [4, 5], if the agent is in the top right grid block the observation would be [5, 5].\n",
    "    \n",
    "    **2.2.** A vector of length 36 representing the one-hot encoding of the integer observation (refer type 1 above). E.g., if the agent is in the bottom left grid block (starting position) the observation would be [1, 0, ..., 0, 0], if the agent is in the grid block containing the Gold the observation would be [0, 0, ..., 1, 0], if the agent is in the top right grid block the observation would be [0, 0, ..., 0, 1].\n",
    "\n",
    "\n",
    "3. **Image** - Image render of the environment returned as an NumPy array. The image size is 84 * 84 (same size used in the DQN paper). E.g., if the agent is in the bottom right grid block the observation is:\n",
    "\n",
    "    Observation: (84 * 84)\n",
    "\n",
    "     [[255 255 255 ... 255 255 255]\n",
    "\n",
    "     [255 255 255 ... 255 255 255]\n",
    "\n",
    "     [255 255 255 ... 255 255 255]\n",
    "\n",
    "     ...\n",
    "\n",
    "     [255 255 255 ... 255 255 255]\n",
    "\n",
    "     [255 255 255 ... 255 255 255]\n",
    "\n",
    "     [255 255 255 ... 255 255 255]]\n",
    "\n",
    "    Observation type: <class 'numpy.ndarray'>\n",
    "\n",
    "    Observation Shape: (84, 84)\n",
    "\n",
    "    Visually, it looks like:\n",
    "    <img src=\"./images/environment_render.png\" width=\"500\" height=\"500\">\n",
    "    \n",
    "\n",
    "4. **Float** - Float in the range [0 - $\\infty$] representing the time elapsed in seconds. \n",
    "\n",
    "#### ENVIRONMENT ACTIONS:\n",
    "\n",
    "Our implementation of the environment provides you with three different types of actions:\n",
    "\n",
    "1. **Discrete** - Integer in the range [0 - 3] representing the four actions possible in the environment as follows: 0 - Right 1 - Left 2 - Up 3 - Down.\n",
    "\n",
    "2. **Multi-Discrete** - Array of length four where each element takes binary values 0 or 1. Array elements represent if we take a particular action. Array element with index 0 corresponds to the right action, index 1 corresponds to the left action, index 2 corresponds to the up action, and index 3 corresponds to the down action. E.g., \n",
    "   action = [1, 0, 0, 0] would result in the agent moving right.\n",
    "   action = [1, 0, 1, 0] would result in the agent moving right and up.\n",
    "   action = [0, 1, 0, 1] would result in the agent moving left and down.\n",
    "\n",
    "3. **Continuous** - Float in the range [-1, 1] determining whether the agent will go left, right, up, or down as follows:\n",
    "\n",
    "    if -1 <= action <= -0.5:\n",
    "        Go Right.\n",
    "    elif -0.5 < action <= 0:\n",
    "        Go Left.\n",
    "    elif 0 < action <= 0.5:\n",
    "        Go Up.\n",
    "    elif 0.5 < action <= 1:\n",
    "        Go Down.\n",
    "        \n",
    "### YOUR TASK IS TO USE A NEURAL NETWORK TO WORK WITH ALL FOUR TYPES OF OBSERVATIONS AND ALL THREE TYPES OF  ACTIONS.\n",
    "### Note: You don't have to train your agent/neural network. You just have to build the neural network structure that takes the observation as input and produces the desired output with the initial weights.\n",
    "\n",
    "#### You can use libraries such as PyTorch/TensorFlow/Keras to build your neural networks.\n",
    "\n",
    "#### <span style=\"color:red\">You cannot use RL libraries that already provide the neural network to you such as Stable-baselines3, Keras-RL, TF agents, Ray RLLib etc.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6deebbb",
   "metadata": {},
   "source": [
    "<img src=\"./images/wumpus_world_environment.jpg\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f11e24",
   "metadata": {},
   "source": [
    "# START COMPLETING YOUR ASSIGNMENT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a8768",
   "metadata": {},
   "source": [
    "## Observation Type - Integer, Action Type - Discrete\n",
    "\n",
    "The part of the assignment requires you to create a sequential dense neural network with 1 hidden layer having 64 neurons and the output layer having 4 neurons. The input to the neural network is an integer (refer to environment observations type 1). The output of the neural network is an array represeting the Q-values from which you will choose an action (refer to environment actions type 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596cb95",
   "metadata": {},
   "source": [
    "The following figure shows the network structure you will have to use:\n",
    "\n",
    "<img src=\"./images/neural_network_structures/neural_network_1_64_4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f4da4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:30:19.791112Z",
     "start_time": "2024-09-24T11:30:19.782559Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"TO DO: Create a neural network, pass it the observation from the environment\n",
    "and get the predicted Q-values for the four actions. Print the observation and the Q-values.\"\"\"\n",
    "\n",
    "environment = WumpusWorldEnvironment(observation_type='integer', action_type='discrete')\n",
    "observation, info = environment.reset()\n",
    "\n",
    "# BEGIN_YOUR_CODE\n",
    "\n",
    "class NeuralNetwork1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork1, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DQN1():\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.env = env\n",
    "        self.D = deque([], maxlen=1000)\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.Q_policy = NeuralNetwork1(len(self.env.observation_space.n), self.env.action_space.n)\n",
    "        self.Q_prime = NeuralNetwork1(len(self.env.observation_space.n), self.env.action_space.n)\n",
    "        self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.1\n",
    "        self.gamma = 0.8\n",
    "\n",
    "        self.C = 10\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.AdamW(self.Q_policy.parameters(), lr=1e-2)\n",
    "\n",
    "    \n",
    "    def epsilon_greedy(self, prev_state):\n",
    "\n",
    "        if self.epsilon > np.random.random():\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            q_values = self.Q_policy(torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)) \n",
    "            action = torch.argmax(q_values).item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "        if len(self.D) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.D, self.batch_size)\n",
    "        \n",
    "        for prev_state, action, reward, next_state, done in batch:\n",
    "            prev_state_tensor = torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            q_values = self.Q_policy(prev_state_tensor)\n",
    "            next_q_values = self.Q_prime(next_state_tensor)\n",
    "            \n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + (self.gamma * torch.max(next_q_values))\n",
    "            \n",
    "            q_value = q_values[0, action]\n",
    "            \n",
    "            loss = self.criterion(q_value, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        for episodes in range(5):\n",
    "\n",
    "            prev_state, info = self.env.reset()\n",
    "\n",
    "            for t in range(5):\n",
    "                \n",
    "                action = self.epsilon_greedy(prev_state)\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "                done = True if terminated or truncated else False\n",
    "\n",
    "                self.D.append((prev_state, action, reward, next_state, done))\n",
    "\n",
    "                self.train_model()\n",
    "\n",
    "                prev_state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * 0.995)\n",
    "\n",
    "            if t%self.C == 0:\n",
    "                self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb8286a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2b233bb",
   "metadata": {},
   "source": [
    "## Observation Type - Vector (2.1), Action Type - Discrete\n",
    "\n",
    "The part of the assignment requires you to create a sequential dense neural network with 1 hidden layer having 64 neurons and the output layer having 4 neurons. The input to the neural network is a vector of length 2 (refer to environment observations type 2.1). The output of the neural network is an array represeting the Q-values from which you will choose an action (refer to environment actions type 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c4873",
   "metadata": {},
   "source": [
    "The following figure shows the network structure you will have to use:\n",
    "\n",
    "<img src=\"./images/neural_network_structures/neural_network_2_64_4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d985b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:30:20.039771Z",
     "start_time": "2024-09-24T11:30:20.032436Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"TO DO: Create a neural network, pass it the observation from the environment\n",
    "and get the predicted Q-values for the four actions. Print the observation and the Q-values.\"\"\"\n",
    "\n",
    "environment = WumpusWorldEnvironment(observation_type='vector', action_type='discrete')\n",
    "observation, info = environment.reset()\n",
    "\n",
    "# BEGIN_YOUR_CODE\n",
    "\n",
    "class NeuralNetwork2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork2, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(2, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DQN2():\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.env = env\n",
    "        self.D = deque([], maxlen=1000)\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.Q_policy = NeuralNetwork2(len(self.env.observation_space.n), self.env.action_space.n)\n",
    "        self.Q_prime = NeuralNetwork2(len(self.env.observation_space.n), self.env.action_space.n)\n",
    "        self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.1\n",
    "        self.gamma = 0.8\n",
    "\n",
    "        self.C = 10\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.AdamW(self.Q_policy.parameters(), lr=1e-2)\n",
    "\n",
    "    \n",
    "    def epsilon_greedy(self, prev_state):\n",
    "\n",
    "        if self.epsilon > np.random.random():\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            q_values = self.Q_policy(torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)) \n",
    "            action = torch.argmax(q_values).item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "        if len(self.D) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.D, self.batch_size)\n",
    "        \n",
    "        for prev_state, action, reward, next_state, done in batch:\n",
    "            prev_state_tensor = torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            q_values = self.Q_policy(prev_state_tensor)\n",
    "            next_q_values = self.Q_prime(next_state_tensor)\n",
    "            \n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + (self.gamma * torch.max(next_q_values))\n",
    "            \n",
    "            q_value = q_values[0, action]\n",
    "            \n",
    "            loss = self.criterion(q_value, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        for episodes in range(5):\n",
    "\n",
    "            prev_state, info = self.env.reset()\n",
    "\n",
    "            for t in range(5):\n",
    "                \n",
    "                action = self.epsilon_greedy(prev_state)\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "                done = True if terminated or truncated else False\n",
    "\n",
    "                self.D.append((prev_state, action, reward, next_state, done))\n",
    "\n",
    "                self.train_model()\n",
    "\n",
    "                prev_state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * 0.995)\n",
    "\n",
    "            if t%self.C == 0:\n",
    "                self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "\n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8957b9f",
   "metadata": {},
   "source": [
    "## Observation Type - Vector (2.2), Action Type - Discrete\n",
    "\n",
    "The part of the assignment requires you to create a sequential dense neural network with 1 hidden layer having 64 neurons and the output layer having 4 neurons. The input to the neural network is a vector of length 36 (refer to environment observations type 2.2). The output of the neural network is an array represeting the Q-values from which you will choose an action (refer to environment actions type 1).\n",
    "\n",
    "**HINT:** Use the integer observation and convert it to a one-hot encoded vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470cf5f8",
   "metadata": {},
   "source": [
    "The following figure shows the network structure you will have to use:\n",
    "\n",
    "<img src=\"./images/neural_network_structures/neural_network_36_64_4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05b09e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:30:20.058979Z",
     "start_time": "2024-09-24T11:30:20.051638Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"TO DO: Create a neural network, pass it the observation from the environment\n",
    "and get the predicted Q-values for the four actions. Print the observation and the Q-values.\"\"\"\n",
    "\n",
    "environment = WumpusWorldEnvironment(observation_type='integer', action_type='discrete')\n",
    "observation, info = environment.reset()\n",
    "\n",
    "# BEGIN_YOUR_CODE\n",
    "\n",
    "class NeuralNetwork3(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork3, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(36, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DQN3():\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.env = env\n",
    "        self.D = deque([], maxlen=1000)\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.Q_policy = NeuralNetwork3(len(self.env.observation_space.n), self.env.action_space.n)\n",
    "        self.Q_prime = NeuralNetwork3(len(self.env.observation_space.n), self.env.action_space.n)\n",
    "        self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.1\n",
    "        self.gamma = 0.8\n",
    "\n",
    "        self.C = 10\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.AdamW(self.Q_policy.parameters(), lr=1e-2)\n",
    "\n",
    "    \n",
    "    def epsilon_greedy(self, prev_state):\n",
    "\n",
    "        if self.epsilon > np.random.random():\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            q_values = self.Q_policy(torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)) \n",
    "            action = torch.argmax(q_values).item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "        if len(self.D) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.D, self.batch_size)\n",
    "        \n",
    "        for prev_state, action, reward, next_state, done in batch:\n",
    "            prev_state_tensor = torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            q_values = self.Q_policy(prev_state_tensor)\n",
    "            next_q_values = self.Q_prime(next_state_tensor)\n",
    "            \n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + (self.gamma * torch.max(next_q_values))\n",
    "            \n",
    "            q_value = q_values[0, action]\n",
    "            \n",
    "            loss = self.criterion(q_value, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        for episodes in range(5):\n",
    "\n",
    "            prev_state, info = self.env.reset()\n",
    "\n",
    "            for t in range(5):\n",
    "                \n",
    "                action = self.epsilon_greedy(prev_state)\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "                done = True if terminated or truncated else False\n",
    "\n",
    "                self.D.append((prev_state, action, reward, next_state, done))\n",
    "\n",
    "                self.train_model()\n",
    "\n",
    "                prev_state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * 0.995)\n",
    "\n",
    "            if t%self.C == 0:\n",
    "                self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19c97b",
   "metadata": {},
   "source": [
    "## Observation Type - Image, Action Type - Discrete\n",
    "\n",
    "The part of the assignment requires you to create a convolutional neural network with one convolutional layer having 128 filters of size 3 x 3, one hidden layer having 64 neurons, and the output layer having 4 neurons. The input to the neural network is an image of size 84 * 84 (refer to environment observations type 3). The output of the neural network is an array represeting the Q-values from which you will choose an action (refer to environment actions type 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d739c",
   "metadata": {},
   "source": [
    "The following figure shows the network structure you will have to use:\n",
    "\n",
    "<img src=\"./images/neural_network_structures/convolutional_neural_network_84x84_128_64_4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2044da09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:30:21.029222Z",
     "start_time": "2024-09-24T11:30:20.071698Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"TO DO: Create a neural network, pass it the observation from the environment\n",
    "and get the predicted Q-values for the four actions. Print the observation and the Q-values.\"\"\"\n",
    "\n",
    "environment = WumpusWorldEnvironment(observation_type='image', action_type='discrete')\n",
    "observation, info = environment.reset()\n",
    "\n",
    "# BEGIN_YOUR_CODE\n",
    "\n",
    "class NeuralNetwork4(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork4, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1 ,128, 3)\n",
    "        self.maxpool = nn.MaxPool2d(3,3)\n",
    "\n",
    "        self.fc1 = nn.Linear(128*28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DQN4():\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.env = env\n",
    "        self.D = deque([], maxlen=1000)\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.Q_policy = NeuralNetwork4(len(self.env.observation_space.n), self.env.action_space.n)\n",
    "        self.Q_prime = NeuralNetwork4(len(self.env.observation_space.n), self.env.action_space.n)\n",
    "        self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.1\n",
    "        self.gamma = 0.8\n",
    "\n",
    "        self.C = 10\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.AdamW(self.Q_policy.parameters(), lr=1e-2)\n",
    "\n",
    "    \n",
    "    def epsilon_greedy(self, prev_state):\n",
    "\n",
    "        if self.epsilon > np.random.random():\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            q_values = self.Q_policy(torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)) \n",
    "            action = torch.argmax(q_values).item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "        if len(self.D) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.D, self.batch_size)\n",
    "        \n",
    "        for prev_state, action, reward, next_state, done in batch:\n",
    "            prev_state_tensor = torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            q_values = self.Q_policy(prev_state_tensor)\n",
    "            next_q_values = self.Q_prime(next_state_tensor)\n",
    "            \n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + (self.gamma * torch.max(next_q_values))\n",
    "            \n",
    "            q_value = q_values[0, action]\n",
    "            \n",
    "            loss = self.criterion(q_value, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        for episodes in range(5):\n",
    "\n",
    "            prev_state, info = self.env.reset()\n",
    "\n",
    "            prev_state = cv2.cvtColor(prev_state, cv2.COLOR_BGR2GRAY)\n",
    "            prev_state = prev_state/255.0\n",
    "\n",
    "            for t in range(5):\n",
    "                \n",
    "                action = self.epsilon_greedy(prev_state)\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "                done = True if terminated or truncated else False\n",
    "\n",
    "                new_state = cv2.cvtColor(new_state, cv2.COLOR_BGR2GRAY)\n",
    "                new_state = new_state/255.0\n",
    "\n",
    "                self.D.append((prev_state, action, reward, next_state, done))\n",
    "\n",
    "                self.train_model()\n",
    "\n",
    "                prev_state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * 0.995)\n",
    "\n",
    "            if t%self.C == 0:\n",
    "                self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be04fd",
   "metadata": {},
   "source": [
    "## Observation Type - Float, Action Type - Discrete\n",
    "\n",
    "The part of the assignment requires you to create a sequential dense neural network with 1 hidden layer having 64 neurons and the output layer having 4 neurons. The input to the neural network is a float (refer to environment observations type 4). The output of the neural network is an array representing the Q-values from which you will choose an action (refer to environment actions type 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed49e0",
   "metadata": {},
   "source": [
    "The following figure shows the network structure you will have to use:\n",
    "\n",
    "<img src=\"./images/neural_network_structures/neural_network_1_64_4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e4aa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:30:21.054512Z",
     "start_time": "2024-09-24T11:30:21.045576Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"TO DO: Create a neural network, pass it the observation from the environment\n",
    "and get the predicted Q-values for the four actions. Print the observation and the Q-values.\"\"\"\n",
    "\n",
    "environment = WumpusWorldEnvironment(observation_type='float', action_type='discrete')\n",
    "observation, info = environment.reset()\n",
    "\n",
    "# BEGIN_YOUR_CODE\n",
    "\n",
    "\n",
    "class NeuralNetwork5(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork5, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DQN5():\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.env = env\n",
    "        self.D = deque([], maxlen=1000)\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.Q_policy = NeuralNetwork5(len(self.env.observation_space.n), self.env.action_space.n)\n",
    "        self.Q_prime = NeuralNetwork5(len(self.env.observation_space.n), self.env.action_space.n)\n",
    "        self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.1\n",
    "        self.gamma = 0.8\n",
    "\n",
    "        self.C = 10\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.AdamW(self.Q_policy.parameters(), lr=1e-2)\n",
    "\n",
    "    \n",
    "    def epsilon_greedy(self, prev_state):\n",
    "\n",
    "        if self.epsilon > np.random.random():\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            q_values = self.Q_policy(torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)) \n",
    "            action = torch.argmax(q_values).item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "        if len(self.D) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.D, self.batch_size)\n",
    "        \n",
    "        for prev_state, action, reward, next_state, done in batch:\n",
    "            prev_state_tensor = torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            q_values = self.Q_policy(prev_state_tensor)\n",
    "            next_q_values = self.Q_prime(next_state_tensor)\n",
    "            \n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + (self.gamma * torch.max(next_q_values))\n",
    "            \n",
    "            q_value = q_values[0, action]\n",
    "            \n",
    "            loss = self.criterion(q_value, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        for episodes in range(5):\n",
    "\n",
    "            prev_state, info = self.env.reset()\n",
    "\n",
    "            for t in range(5):\n",
    "                \n",
    "                action = self.epsilon_greedy(prev_state)\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "                done = True if terminated or truncated else False\n",
    "\n",
    "                self.D.append((prev_state, action, reward, next_state, done))\n",
    "\n",
    "                self.train_model()\n",
    "\n",
    "                prev_state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * 0.995)\n",
    "\n",
    "            if t%self.C == 0:\n",
    "                self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27040465",
   "metadata": {},
   "source": [
    "## Observation Type - Vector (2.2), Action Type - Multi-Discrete\n",
    "\n",
    "The part of the assignment requires you to create a sequential dense neural network with 1 hidden layer having 64 neurons and the output layer having 4 neurons. The input to the neural network is a vector of length 36 (refer to environment observations type 2.2). The output of the neural network is an array representing the probability of choosing the actions. (If the value of the array element is >=0.5 you will perform the action.) (refer to environment actions type 2).\n",
    "\n",
    "**HINT:** Use the integer observation and convert it to a one-hot encoded vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe64de9",
   "metadata": {},
   "source": [
    "The following figure shows the network structure you will have to use:\n",
    "\n",
    "<img src=\"./images/neural_network_structures/neural_network_36_64_4_sigmoid.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea3736",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:30:21.075465Z",
     "start_time": "2024-09-24T11:30:21.067936Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"TO DO: Create a neural network, pass it the observation from the environment\n",
    "and get the predicted action probabilities for the four actions. Print the observation and the action probabilities.\"\"\"\n",
    "\n",
    "environment = WumpusWorldEnvironment(observation_type='integer', action_type='multi_discrete')\n",
    "observation, info = environment.reset()\n",
    "\n",
    "# BEGIN_YOUR_CODE\n",
    "class NeuralNetwork6(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork6, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(36, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DQN6():\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.env = env\n",
    "        self.D = deque([], maxlen=1000)\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.Q_policy = NeuralNetwork6(len(self.env.observation_space.n), self.env.action_space.n)\n",
    "        self.Q_prime = NeuralNetwork6(len(self.env.observation_space.n), self.env.action_space.n)\n",
    "        self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.1\n",
    "        self.gamma = 0.8\n",
    "\n",
    "        self.C = 10\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.AdamW(self.Q_policy.parameters(), lr=1e-2)\n",
    "\n",
    "    \n",
    "    def epsilon_greedy(self, prev_state):\n",
    "\n",
    "        if self.epsilon > np.random.random():\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            q_values = self.Q_policy(torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)) \n",
    "            action = torch.argmax(q_values).item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "        if len(self.D) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.D, self.batch_size)\n",
    "        \n",
    "        for prev_state, action, reward, next_state, done in batch:\n",
    "            prev_state_tensor = torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            q_values = self.Q_policy(prev_state_tensor)\n",
    "            next_q_values = self.Q_prime(next_state_tensor)\n",
    "            \n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + (self.gamma * torch.max(next_q_values))\n",
    "            \n",
    "            q_value = q_values[0, action]\n",
    "            \n",
    "            loss = self.criterion(q_value, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        for episodes in range(5):\n",
    "\n",
    "            prev_state, info = self.env.reset()\n",
    "\n",
    "            for t in range(5):\n",
    "                \n",
    "                action = self.epsilon_greedy(prev_state)\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "                done = True if terminated or truncated else False\n",
    "\n",
    "                self.D.append((prev_state, action, reward, next_state, done))\n",
    "\n",
    "                self.train_model()\n",
    "\n",
    "                prev_state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * 0.995)\n",
    "\n",
    "            if t%self.C == 0:\n",
    "                self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f05b0",
   "metadata": {},
   "source": [
    "## Observation Type - Vector (2.2), Action Type - Continuous\n",
    "\n",
    "The part of the assignment requires you to create a sequential dense neural network with 1 hidden layer having 64 neurons and the output layer having 1 neuron. The input to the neural network is a vector of length 36 (refer to environment observations type 2.2). The output of the neural network is an float in the range [-1, 1] determining the action which will be taken. (refer to environment actions type 3).\n",
    "\n",
    "**HINT:** Use the integer observation and convert it to a one-hot encoded vector and use the TanH activation function to get the output in the range [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8796988",
   "metadata": {},
   "source": [
    "The following figure shows the network structure you will have to use:\n",
    "\n",
    "<img src=\"./images/neural_network_structures/neural_network_36_64_1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98555f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:30:21.123624Z",
     "start_time": "2024-09-24T11:30:21.114457Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"TO DO: Create a neural network, pass it the observation from the environment\n",
    "and get the predicted action. Print the observation and the predicted action.\"\"\"\n",
    "\n",
    "environment = WumpusWorldEnvironment(observation_type='integer', action_type='continuous')\n",
    "observation, info = environment.reset()\n",
    "\n",
    "# BEGIN_YOUR_CODE\n",
    "\n",
    "class NeuralNetwork7(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork7, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(36, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DQN7():\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.env = env\n",
    "        self.D = deque([], maxlen=1000)\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.Q_policy = NeuralNetwork7(len(self.env.observation_space.n), self.env.action_space.n)\n",
    "        self.Q_prime = NeuralNetwork7(len(self.env.observation_space.n), self.env.action_space.n)\n",
    "        self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.1\n",
    "        self.gamma = 0.8\n",
    "\n",
    "        self.C = 10\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.AdamW(self.Q_policy.parameters(), lr=1e-2)\n",
    "\n",
    "    \n",
    "    def epsilon_greedy(self, prev_state):\n",
    "\n",
    "        if self.epsilon > np.random.random():\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            q_values = self.Q_policy(torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)) \n",
    "            action = torch.argmax(q_values).item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "        if len(self.D) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.D, self.batch_size)\n",
    "        \n",
    "        for prev_state, action, reward, next_state, done in batch:\n",
    "            prev_state_tensor = torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            q_values = self.Q_policy(prev_state_tensor)\n",
    "            next_q_values = self.Q_prime(next_state_tensor)\n",
    "            \n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + (self.gamma * torch.max(next_q_values))\n",
    "            \n",
    "            q_value = q_values[0, action]\n",
    "            \n",
    "            loss = self.criterion(q_value, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        for episodes in range(5):\n",
    "\n",
    "            prev_state, info = self.env.reset()\n",
    "\n",
    "            for t in range(5):\n",
    "                \n",
    "                action = self.epsilon_greedy(prev_state)\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "                done = True if terminated or truncated else False\n",
    "\n",
    "                self.D.append((prev_state, action, reward, next_state, done))\n",
    "\n",
    "                self.train_model()\n",
    "\n",
    "                prev_state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * 0.995)\n",
    "\n",
    "            if t%self.C == 0:\n",
    "                self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "# END_YOUR_CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
