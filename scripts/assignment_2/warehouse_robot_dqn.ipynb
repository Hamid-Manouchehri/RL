{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "898d386e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n*******************************************************************************\\n\\nProject: RL, assignment 1\\nFile: warehouse_robot_Q_learning.ipynb\\nAuthor: Hamid Manouchehri\\nEmail: hmanouch@buffalo.edu\\nDate: Feb 7, 2025\\n\\nDescription:\\nCheckpoint 1, preparing an RL environment\\nEnvironmet Scenario: Warehouse Robot\\n\\nLicense:\\nThis script is licensed under the MIT License.\\nYou may obtain a copy of the License at\\n    https://opensource.org/licenses/MIT\\n\\nSPDX-License-Identifier: MIT\\n\\nDisclaimer:\\nThis software is provided \"as is\", without warranty of any kind, express or\\nimplied, including but not limited to the warranties of merchantability,\\nfitness for a particular purpose, and noninfringement. In no event shall the\\nauthors be liable for any claim, damages, or other liability, whether in an\\naction of contract, tort, or otherwise, arising from, out of, or in connection\\nwith the software or the use or other dealings in the software.\\n\\n*******************************************************************************\\n'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "*******************************************************************************\n",
    "\n",
    "Project: RL, assignment 1\n",
    "File: warehouse_robot_Q_learning.ipynb\n",
    "Author: Hamid Manouchehri\n",
    "Email: hmanouch@buffalo.edu\n",
    "Date: Feb 7, 2025\n",
    "\n",
    "Description:\n",
    "Checkpoint 1, preparing an RL environment\n",
    "Environmet Scenario: Warehouse Robot\n",
    "\n",
    "License:\n",
    "This script is licensed under the MIT License.\n",
    "You may obtain a copy of the License at\n",
    "    https://opensource.org/licenses/MIT\n",
    "\n",
    "SPDX-License-Identifier: MIT\n",
    "\n",
    "Disclaimer:\n",
    "This software is provided \"as is\", without warranty of any kind, express or\n",
    "implied, including but not limited to the warranties of merchantability,\n",
    "fitness for a particular purpose, and noninfringement. In no event shall the\n",
    "authors be liable for any claim, damages, or other liability, whether in an\n",
    "action of contract, tort, or otherwise, arising from, out of, or in connection\n",
    "with the software or the use or other dealings in the software.\n",
    "\n",
    "*******************************************************************************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b9a9f732-125d-43a5-b84b-9a0d4b5b5973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (0.28.1)\n",
      "Requirement already satisfied: matplotlib in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (3.5.3)\n",
      "Requirement already satisfied: numpy in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (1.21.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (from gymnasium) (2.0.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (from gymnasium) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (from gymnasium) (4.7.1)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (from gymnasium) (1.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (from matplotlib) (22.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/hamid/miniconda3/envs/rl_env/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f818e407-cfa9-46ca-9bdc-99aa0d2aea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.colors as mcolors\n",
    "from IPython.display import clear_output\n",
    "# from environment import WumpusWorldEnvironment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "639419a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "## Custom Function Definition ##\n",
    "################################\n",
    "def save_plot_as_img(png_plot_name, dir):\n",
    "    \"\"\" Call this function after plt.plot(...)\"\"\"\n",
    "    \n",
    "    save_path = os.path.join(dir, png_plot_name)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    print(f\"Plot saved at: {save_path}\")\n",
    "\n",
    "\n",
    "def get_state_index(obs):\n",
    "\n",
    "    index_lookup_table = np.arange(72).reshape((2,6,6))\n",
    "    \n",
    "    return index_lookup_table[obs[0], obs[1], obs[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e1106490-e2d4-4026-ba0f-7f743df10353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_size = 6\n",
    "grid = np.ones((grid_size, grid_size, 3))\n",
    "# Define grid size\n",
    "grid_size = 6  # 6x6 grid\n",
    "\n",
    "# Initialize grid with default background (White = 4)\n",
    "grid = np.full((grid_size, grid_size), 1, dtype=int)  # Default to background\n",
    "\n",
    "# Define agent and goal positions\n",
    "init_agent_pos = [0, 0]  # Start position\n",
    "goal_pos = [5, 5]  # Goal position\n",
    "\n",
    "# Define integer-based color mapping\n",
    "rgb_colors = {\n",
    "    \"black\": 0,   # Agent\n",
    "    \"white\": 1,   # Goal\n",
    "    \"red\": 2,     # Trap\n",
    "    \"blue\": 3,    # Water\n",
    "    \"green\": 4,   # Background\n",
    "    \"gray\": 5     # Obstacle\n",
    "}\n",
    "\n",
    "# Define color list for visualization\n",
    "color_map = {\n",
    "    0: \"black\",\n",
    "    1: \"white\",\n",
    "    2: \"red\",\n",
    "    3: \"blue\",\n",
    "    4: \"green\",\n",
    "    5: \"gray\"\n",
    "}\n",
    "\n",
    "agent_obj_state = {\n",
    "    0: \"no_obj\",\n",
    "    1: \"have_obj\"\n",
    "}\n",
    "\n",
    "# Assign integer values to the grid\n",
    "grid[tuple(init_agent_pos)] = rgb_colors[\"black\"]  # Agent\n",
    "grid[tuple(goal_pos)] = rgb_colors[\"green\"]  # Goal\n",
    "\n",
    "# Create a colormap using actual colors\n",
    "cmap = mcolors.ListedColormap([color_map[i] for i in range(len(color_map))])\n",
    "\n",
    "for i in range(6):  # rows\n",
    "    for j in range(6):  # columns\n",
    "        text = \"\"\n",
    "        if [i, j] == init_agent_pos:\n",
    "            text = \"Agent\"\n",
    "        elif [i, j] == goal_pos:\n",
    "            text = \"Goal\"\n",
    "        \n",
    "        # Only annotate if there is text to display\n",
    "        if text:\n",
    "            plt.text(j, i, text, ha=\"center\", va=\"center\", color=\"white\", fontsize=8)\n",
    "\n",
    "\n",
    "# Display the grid with correct colors\n",
    "plt.imshow(grid, cmap=cmap, vmin=0, vmax=len(color_map) - 1)\n",
    "plt.title(\"Grid Environment\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6ebcf2b4-c920-4a97-9878-5299967e9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the Grid Environment class.\n",
    "\n",
    "class GridEnvironment(gym.Env):\n",
    "    # Attribute of a Gym class that provides info about the render modes\n",
    "    metadata = { 'render.modes': [] }\n",
    "\n",
    "    # Initialization function\n",
    "    def __init__(self, max_timesteps):\n",
    "        # Initializes the class\n",
    "        # Define action and observation space\n",
    "        self.observation_space = spaces.Discrete(36)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.max_timesteps = max_timesteps  # TODO\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.timestep = 0\n",
    "        self.agent_carry_obj = False\n",
    "        self.agent_pos = [0,0]\n",
    "        self.goal_pos = [5,5]\n",
    "        self.object_pos = [1, 2]\n",
    "        self.obstacle_1_pos = [2, 2]\n",
    "\n",
    "        self.state = np.ones((6,6))\n",
    "        self.state[tuple(self.agent_pos)] = rgb_colors[\"black\"]\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"green\"]\n",
    "        self.state[tuple(self.object_pos)] = rgb_colors[\"blue\"]\n",
    "        self.state[tuple(self.obstacle_1_pos)] = rgb_colors[\"red\"]\n",
    "\n",
    "\n",
    "    # Reset function\n",
    "    def reset(self, **kwargs):\n",
    "        self.agent_carry_obj = False\n",
    "        self.agent_pos = [0,0]\n",
    "        self.goal_pos = [5,5]\n",
    "        self.object_pos = [1, 2]\n",
    "        self.obstacle_1_pos = [2, 2]\n",
    "        self.reward = 0\n",
    "        self.timestep = 0\n",
    "        self.state = np.ones((6,6))\n",
    "        self.state[tuple(self.agent_pos)] = rgb_colors[\"black\"]\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"green\"]\n",
    "        self.state[tuple(self.object_pos)] = rgb_colors[\"blue\"]\n",
    "        self.state[tuple(self.obstacle_1_pos)] = rgb_colors[\"red\"]\n",
    "        observation = np.append(int(self.agent_carry_obj),np.array(self.agent_pos))\n",
    "\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        return observation, info\n",
    "\n",
    "\n",
    "    def pick_up_obj(self):\n",
    "        self.state[tuple(self.object_pos)] = rgb_colors[\"white\"]\n",
    "        self.agent_carry_obj = True\n",
    "\n",
    "\n",
    "    def drop_off_obj(self):\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"white\"]\n",
    "        self.agent_carry_obj = False\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        self.reward -= 1\n",
    "        terminated = False\n",
    "\n",
    "        # Compute the potential new position based on the action.\n",
    "        new_agent_pos = self.agent_pos.copy()\n",
    "        if action == 0:  # down\n",
    "            new_agent_pos[0] += 1\n",
    "        elif action == 1:  # up\n",
    "            new_agent_pos[0] -= 1\n",
    "        elif action == 2:  # right\n",
    "            new_agent_pos[1] += 1\n",
    "        elif action == 3:  # left\n",
    "            new_agent_pos[1] -= 1\n",
    "\n",
    "        # Ensure the new position is within bounds.\n",
    "        new_agent_pos = np.clip(new_agent_pos, 0, 5).tolist()\n",
    "\n",
    "        # Obstacle avoidance: If the new position is the obstacle, do not update agent_pos.\n",
    "        if np.array_equal(new_agent_pos, self.obstacle_1_pos):\n",
    "            self.reward -= 25  # Apply penalty\n",
    "            # Optionally, you might decide to leave the agent in place:\n",
    "            # new_agent_pos remains as the current position.\n",
    "        else:\n",
    "            # Otherwise, update the agent's position.\n",
    "            self.agent_pos = new_agent_pos\n",
    "\n",
    "        # Update the grid state.\n",
    "        self.state = np.ones((6,6))\n",
    "        self.state[tuple(self.agent_pos)] = rgb_colors[\"black\"]\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"green\"]\n",
    "        if not self.agent_carry_obj:\n",
    "            self.state[tuple(self.object_pos)] = rgb_colors[\"blue\"]\n",
    "        self.state[tuple(self.obstacle_1_pos)] = rgb_colors[\"red\"]\n",
    "\n",
    "        # Create the observation.\n",
    "        observation = np.append(int(self.agent_carry_obj), np.array(self.agent_pos))\n",
    "\n",
    "        # Check for picking up the object.\n",
    "        if np.array_equal(self.agent_pos, self.object_pos) and (self.agent_carry_obj == False):\n",
    "            self.pick_up_obj()\n",
    "            self.reward += 25\n",
    "            self.state[tuple(self.object_pos)] = rgb_colors[\"black\"]\n",
    "\n",
    "        # Check for dropping off the object.\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos) and (self.agent_carry_obj == True):\n",
    "            self.drop_off_obj()\n",
    "            self.reward += 100\n",
    "            terminated = True\n",
    "            self.state[tuple(self.goal_pos)] = rgb_colors[\"black\"]\n",
    "\n",
    "        # Termination condition based on timestep.\n",
    "        self.timestep += 1\n",
    "        if self.timestep >= self.max_timesteps:\n",
    "            terminated = True\n",
    "\n",
    "        # Check if agent remains within permitted cells.\n",
    "        if np.all((np.asarray(self.agent_pos) >= 0) & (np.asarray(self.agent_pos) <= 5)):\n",
    "            truncated = True\n",
    "        else:\n",
    "            truncated = False\n",
    "\n",
    "        info = {}\n",
    "        return observation, self.reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "\n",
    "    # Render function: Visualizes the environment\n",
    "    def render(self):\n",
    "\n",
    "        cmap = mcolors.ListedColormap([color_map[i] for i in range(len(color_map))])\n",
    "        plt.imshow(self.state, cmap=cmap, vmin=0, vmax=len(color_map) - 1)\n",
    "        \n",
    "        # Annotate the grid with text labels\n",
    "        for i in range(6):  # rows\n",
    "            for j in range(6):  # columns\n",
    "                label = \"\"\n",
    "                if np.array_equal([i, j], self.agent_pos):\n",
    "                    label = \"Agent\"\n",
    "                elif np.array_equal([i, j], self.goal_pos):\n",
    "                    label = \"Goal\"\n",
    "                elif np.array_equal([i, j], self.object_pos):\n",
    "                    label = \"Obj\"\n",
    "                elif np.array_equal([i, j], self.obstacle_1_pos):\n",
    "                    label = \"Obs\"\n",
    "                \n",
    "                if label:\n",
    "                    plt.text(j, i, label, ha=\"center\", va=\"center\", color=\"white\", fontsize=8)\n",
    "        \n",
    "        plt.title(\"Grid Environment\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0ae16edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your neural network architecture.\n",
    "class NeuralNetwork1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork1, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(3, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DQN1():\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.env = env\n",
    "        self.D = deque([], maxlen=10000)\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.Q_policy = NeuralNetwork1()\n",
    "        self.Q_prime = NeuralNetwork1()\n",
    "        self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.1\n",
    "        self.gamma = 0.9\n",
    "        self.rewards = []\n",
    "\n",
    "        self.C = 100\n",
    "        self.total_steps = 0\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.AdamW(self.Q_policy.parameters(), lr=1e-2)\n",
    "\n",
    "    \n",
    "    def epsilon_greedy(self, prev_state):\n",
    "\n",
    "        if self.epsilon > np.random.random():\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            q_values = self.Q_policy(torch.tensor(prev_state, dtype=torch.float32).unsqueeze(0)) \n",
    "            action = torch.argmax(q_values).item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "        if len(self.D) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.D, self.batch_size)\n",
    "        \n",
    "        states = torch.tensor([s[0] for s in batch], dtype=torch.float32)\n",
    "        actions = torch.tensor([s[1] for s in batch], dtype=torch.long).unsqueeze(1)\n",
    "        rewards = torch.tensor([s[2] for s in batch], dtype=torch.float32)\n",
    "        next_states = torch.tensor([s[3] for s in batch], dtype=torch.float32)\n",
    "        dones = torch.tensor([s[4] for s in batch], dtype=torch.float32)\n",
    "        \n",
    "        current_q_values = self.Q_policy(states).gather(1, actions)\n",
    "        \n",
    "        next_q_values = self.Q_prime(next_states).max(1)[0]\n",
    "        \n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        target_q_values = target_q_values.unsqueeze(1)\n",
    "        \n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        for episodes in range(750):\n",
    "\n",
    "            prev_state, info = self.env.reset()\n",
    "            ep_reward = 0\n",
    "\n",
    "            for t in range(1000):\n",
    "                \n",
    "                action = self.epsilon_greedy(prev_state)\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "                ep_reward+=reward\n",
    "\n",
    "                done = True if terminated or truncated else False\n",
    "\n",
    "                self.D.append((prev_state, action, reward, next_state, done))\n",
    "\n",
    "                self.train_model()\n",
    "\n",
    "                prev_state = next_state\n",
    "                self.total_steps += 1\n",
    "\n",
    "                if t%self.C == 0:\n",
    "                    self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            self.rewards.append(ep_reward)\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * 0.995)\n",
    "\n",
    "\n",
    "\n",
    "env = GridEnvironment(max_timesteps=500)\n",
    "# observation, info = env.reset()\n",
    "dqn_agent = DQN1(env)\n",
    "dqn_agent.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "10e134fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'total reward per episode, test agent, greedy actions')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(nn, num_episodes=10, max_steps=50):\n",
    "    from IPython.display import clear_output\n",
    "    import time\n",
    "    \n",
    "    rewards_all_episodes = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        total_rewards = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Convert observation to a tensor and add batch dimension.\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            # Compute action using the network.\n",
    "            action = torch.argmax(nn(obs_tensor)).item()\n",
    "\n",
    "            # print(\"obs_tensor:\", obs_tensor, \" action: \", action)\n",
    "            \n",
    "            # Execute action.\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_rewards += reward\n",
    "            \n",
    "            # Render the environment.\n",
    "            env.render()\n",
    "            time.sleep(0.05)  # Pause briefly so updates are visible.\n",
    "            clear_output(wait=True)  # Clear previous output.\n",
    "            \n",
    "            if terminated:\n",
    "                break\n",
    "        \n",
    "        rewards_all_episodes.append(total_rewards)\n",
    "    \n",
    "    return rewards_all_episodes\n",
    "\n",
    "\n",
    "t = test(dqn_agent.Q_policy, num_episodes=1, max_steps=200)\n",
    "plt.figure(1)\n",
    "plt.plot(t)\n",
    "plt.title('total reward per episode, test agent, greedy actions')\n",
    "# save_plot_as_img('q_learning_deterministic_test_agent.png', dir_to_save_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
