{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "898d386e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n*******************************************************************************\\n\\nProject: RL, assignment 1\\nFile: warehouse_robot_env.ipynb\\nAuthor: Hamid Manouchehri\\nEmail: hmanouch@buffalo.edu\\nDate: Feb 7, 2025\\n\\nDescription:\\nCheckpoint 1, preparing an RL environment\\nEnvironmet Scenario: Warehouse Robot\\n\\nLicense:\\nThis script is licensed under the MIT License.\\nYou may obtain a copy of the License at\\n    https://opensource.org/licenses/MIT\\n\\nSPDX-License-Identifier: MIT\\n\\nDisclaimer:\\nThis software is provided \"as is\", without warranty of any kind, express or\\nimplied, including but not limited to the warranties of merchantability,\\nfitness for a particular purpose, and noninfringement. In no event shall the\\nauthors be liable for any claim, damages, or other liability, whether in an\\naction of contract, tort, or otherwise, arising from, out of, or in connection\\nwith the software or the use or other dealings in the software.\\n\\n*******************************************************************************\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "*******************************************************************************\n",
    "\n",
    "Project: RL, assignment 1\n",
    "File: warehouse_robot_env.ipynb\n",
    "Author: Hamid Manouchehri\n",
    "Email: hmanouch@buffalo.edu\n",
    "Date: Feb 7, 2025\n",
    "\n",
    "Description:\n",
    "Checkpoint 1, preparing an RL environment\n",
    "Environmet Scenario: Warehouse Robot\n",
    "\n",
    "License:\n",
    "This script is licensed under the MIT License.\n",
    "You may obtain a copy of the License at\n",
    "    https://opensource.org/licenses/MIT\n",
    "\n",
    "SPDX-License-Identifier: MIT\n",
    "\n",
    "Disclaimer:\n",
    "This software is provided \"as is\", without warranty of any kind, express or\n",
    "implied, including but not limited to the warranties of merchantability,\n",
    "fitness for a particular purpose, and noninfringement. In no event shall the\n",
    "authors be liable for any claim, damages, or other liability, whether in an\n",
    "action of contract, tort, or otherwise, arising from, out of, or in connection\n",
    "with the software or the use or other dealings in the software.\n",
    "\n",
    "*******************************************************************************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9a9f732-125d-43a5-b84b-9a0d4b5b5973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (0.28.1)\n",
      "Requirement already satisfied: matplotlib in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (3.10.0)\n",
      "Requirement already satisfied: numpy in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (2.2.2)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (from gymnasium) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/hamid/miniconda3/envs/rl_env/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f818e407-cfa9-46ca-9bdc-99aa0d2aea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1106490-e2d4-4026-ba0f-7f743df10353",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 6\n",
    "grid = np.ones((grid_size, grid_size, 3))\n",
    "# Define grid size\n",
    "grid_size = 6  # 6x6 grid\n",
    "\n",
    "# Initialize grid with default background (White = 4)\n",
    "grid = np.full((grid_size, grid_size), 1, dtype=int)  # Default to background\n",
    "\n",
    "# Define agent and goal positions\n",
    "init_agent_pos = [0, 0]  # Start position\n",
    "goal_pos = [5, 5]  # Goal position\n",
    "\n",
    "# Define integer-based color mapping\n",
    "rgb_colors = {\n",
    "    \"black\": 0,   # Agent\n",
    "    \"white\": 1,   # Goal\n",
    "    \"red\": 2,     # Trap\n",
    "    \"blue\": 3,    # Water\n",
    "    \"green\": 4,   # Background\n",
    "    \"gray\": 5     # Obstacle\n",
    "}\n",
    "\n",
    "# Define color list for visualization\n",
    "color_map = {\n",
    "    0: \"black\",\n",
    "    1: \"white\",\n",
    "    2: \"red\",\n",
    "    3: \"blue\",\n",
    "    4: \"green\",\n",
    "    5: \"gray\"\n",
    "}\n",
    "\n",
    "agent_obj_state = {\n",
    "    0: \"no_obj\",\n",
    "    1: \"have_obj\"\n",
    "}\n",
    "\n",
    "# Assign integer values to the grid\n",
    "grid[tuple(init_agent_pos)] = rgb_colors[\"black\"]  # Agent\n",
    "grid[tuple(goal_pos)] = rgb_colors[\"green\"]  # Goal\n",
    "\n",
    "# Create a colormap using actual colors\n",
    "cmap = mcolors.ListedColormap([color_map[i] for i in range(len(color_map))])\n",
    "\n",
    "for i in range(6):  # rows\n",
    "    for j in range(6):  # columns\n",
    "        text = \"\"\n",
    "        if [i, j] == init_agent_pos:\n",
    "            text = \"Agent\"\n",
    "        elif [i, j] == goal_pos:\n",
    "            text = \"Goal\"\n",
    "        \n",
    "        # Only annotate if there is text to display\n",
    "        if text:\n",
    "            plt.text(j, i, text, ha=\"center\", va=\"center\", color=\"white\", fontsize=8)\n",
    "\n",
    "\n",
    "# Display the grid with correct colors\n",
    "plt.imshow(grid, cmap=cmap, vmin=0, vmax=len(color_map) - 1)\n",
    "plt.title(\"Grid Environment\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6ebcf2b4-c920-4a97-9878-5299967e9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the Grid Environment class.\n",
    "\n",
    "class GridEnvironment(gym.Env):\n",
    "    # Attribute of a Gym class that provides info about the render modes\n",
    "    metadata = { 'render.modes': [] }\n",
    "\n",
    "    # Initialization function\n",
    "    def __init__(self):\n",
    "        # Initializes the class\n",
    "        # Define action and observation space\n",
    "        self.observation_space = spaces.Discrete(36)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.max_timesteps = 100\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.timestep = 0\n",
    "        self.agent_carry_obj = False\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [5, 5]\n",
    "        self.object_pos = [1, 2]\n",
    "        self.obstacle_1_pos = [2, 2]\n",
    "\n",
    "        self.state = np.ones((6,6))\n",
    "        self.state[tuple(self.agent_pos)] = rgb_colors[\"black\"]\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"green\"]\n",
    "        self.state[tuple(self.object_pos)] = rgb_colors[\"blue\"]\n",
    "        self.state[tuple(self.obstacle_1_pos)] = rgb_colors[\"red\"]\n",
    "\n",
    "\n",
    "    # Reset function\n",
    "    def reset(self, **kwargs):\n",
    "        self.agent_carry_obj = False\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [5, 5]\n",
    "        self.object_pos = [1, 2]\n",
    "        self.obstacle_1_pos = [2, 2]\n",
    "        self.reward = 0\n",
    "        self.timestep = 0\n",
    "        self.state = np.ones((6,6))\n",
    "        self.state[tuple(self.agent_pos)] = rgb_colors[\"black\"]\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"green\"]\n",
    "        self.state[tuple(self.object_pos)] = rgb_colors[\"blue\"]\n",
    "        self.state[tuple(self.obstacle_1_pos)] = rgb_colors[\"red\"]\n",
    "        observation = np.append(int(self.agent_carry_obj),np.array(self.agent_pos))\n",
    "\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        return observation, info\n",
    "\n",
    "\n",
    "    def pick_up_obj(self):\n",
    "        self.state[tuple(self.object_pos)] = rgb_colors[\"white\"]\n",
    "        self.agent_carry_obj = True\n",
    "\n",
    "\n",
    "    def drop_off_obj(self):\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"white\"]\n",
    "        self.agent_carry_obj = False\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        self.reward -= 1\n",
    "        # Executes one timestep within the environment\n",
    "        # Input to the function is an action\n",
    "        \n",
    "        if action == 0:  # down\n",
    "            self.agent_pos[0] += 1\n",
    "            \n",
    "        if action == 1:  # up\n",
    "            self.agent_pos[0] -= 1\n",
    "            \n",
    "        if action == 2:  # right:\n",
    "            self.agent_pos[1] += 1\n",
    "            \n",
    "        if action == 3:  # left:\n",
    "            self.agent_pos[1] -= 1\n",
    "    \n",
    "\n",
    "        # Comment this to demonstrate the truncation condition.\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, 5)\n",
    "        \n",
    "        self.state = np.ones((6,6))\n",
    "        self.state[tuple(self.agent_pos)] = rgb_colors[\"black\"]\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"green\"]\n",
    "        self.state[tuple(self.object_pos)] = rgb_colors[\"blue\"]\n",
    "        self.state[tuple(self.obstacle_1_pos)] = rgb_colors[\"red\"]\n",
    "        observation = np.append(int(self.agent_carry_obj),np.array(self.agent_pos))\n",
    "\n",
    "\n",
    "        if np.array_equal(self.agent_pos, self.object_pos) and (self.agent_carry_obj == False):\n",
    "            self.pick_up_obj()\n",
    "            self.reward += 25\n",
    "\n",
    "\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos) and (self.agent_carry_obj == True):\n",
    "            self.drop_off_obj()\n",
    "            self.reward += 100\n",
    "            \n",
    "\n",
    "        if np.array_equal(self.agent_pos, self.obstacle_1_pos):\n",
    "            self.reward -= 25\n",
    "        \n",
    "        self.timestep += 1\n",
    "        \n",
    "\n",
    "        # Condition to check for termination (episode is over)\n",
    "        if self.timestep >= self.max_timesteps:\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "        \n",
    "        print(\"inside step, agent location: \", self.agent_pos)\n",
    "\n",
    "        # Condition to check if agent is traversing to a cell beyond the permitted cells\n",
    "        # This helps the agent to learn how to behave in a safe and predictable manner\n",
    "        if np.all((np.asarray(self.agent_pos) >= 0) & (np.asarray(self.agent_pos) <= 5)):\n",
    "            truncated = True\n",
    "        else:\n",
    "            truncated = False\n",
    "\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        return observation, self.reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    # Render function: Visualizes the environment\n",
    "    def render(self):\n",
    "\n",
    "        cmap = mcolors.ListedColormap([color_map[i] for i in range(len(color_map))])\n",
    "        plt.imshow(self.state, cmap=cmap, vmin=0, vmax=len(color_map) - 1)\n",
    "        \n",
    "        # Annotate the grid with text labels\n",
    "        for i in range(6):  # rows\n",
    "            for j in range(6):  # columns\n",
    "                label = \"\"\n",
    "                if np.array_equal([i, j], self.agent_pos):\n",
    "                    label = \"Agent\"\n",
    "                elif np.array_equal([i, j], self.goal_pos):\n",
    "                    label = \"Goal\"\n",
    "                elif np.array_equal([i, j], self.object_pos):\n",
    "                    label = \"Obj\"\n",
    "                elif np.array_equal([i, j], self.obstacle_1_pos):\n",
    "                    label = \"Obs\"\n",
    "                \n",
    "                if label:\n",
    "                    plt.text(j, i, label, ha=\"center\", va=\"center\", color=\"white\", fontsize=8)\n",
    "                    \n",
    "        plt.title(\"Grid Environment\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9d7c8793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes=1000, max_steps=100, alpha=0.1, gamma=0.99,\n",
    "               epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.995):\n",
    "        \"\"\"\n",
    "        Performs Q-learning on a given environment.\n",
    "        \n",
    "        Parameters:\n",
    "        env: The environment instance (should have reset, step, and get_state_index-like behavior)\n",
    "        num_episodes: Number of episodes for training.\n",
    "        max_steps: Maximum steps per episode.\n",
    "        alpha: Learning rate.\n",
    "        gamma: Discount factor.\n",
    "        epsilon: Initial exploration rate.\n",
    "        epsilon_min: Minimum exploration rate.\n",
    "        epsilon_decay: Decay factor for epsilon after each episode.\n",
    "        \n",
    "        Returns:\n",
    "        Q: The trained Q-table.\n",
    "        rewards_all_episodes: List with total rewards per episode.\n",
    "        \"\"\"\n",
    "        index_lookup_table = np.arange(72).reshape((2,6,6))\n",
    "        grid_size = 6\n",
    "        num_states = grid_size * grid_size\n",
    "        num_actions = env.action_space.n  # 4 actions\n",
    "        \n",
    "        # Initialize Q-table with zeros.\n",
    "        Q_table = np.zeros((num_states*2, num_actions))\n",
    "        rewards_all_episodes = []\n",
    "        \n",
    "        def get_state_index(obs):\n",
    "            \n",
    "            # print(type(obs))\n",
    "            return index_lookup_table[obs[0],obs[1],obs[2]]\n",
    "            \n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            obs, info = env.reset()\n",
    "            state_idx = get_state_index(obs)\n",
    "            total_rewards = 0\n",
    "            for step in range(max_steps):\n",
    "                # Epsilon-greedy action selection.\n",
    "                    # if np.random.rand() < .1:\n",
    "                    #     action = env.action_space.sample()\n",
    "                    # elif\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(Q_table[state_idx, :])\n",
    "\n",
    "                print(\"agent location: \", env.agent_pos, \"action\", action)\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                total_rewards += reward\n",
    "                new_state_idx = get_state_index(obs)\n",
    "                \n",
    "                # Q-learning update rule.\n",
    "                Q_table[state_idx, action] += alpha * (\n",
    "                    reward + gamma * np.max(Q_table[new_state_idx, :]) - Q_table[state_idx, action]\n",
    "                )\n",
    "                \n",
    "                state_idx = new_state_idx\n",
    "                \n",
    "                if terminated:# or truncated:\n",
    "                    break\n",
    "            \n",
    "            # Decay epsilon after each episode.\n",
    "            epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "            rewards_all_episodes.append(total_rewards)\n",
    "        \n",
    "        return Q_table, rewards_all_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485161be-db60-41f5-af64-77d69f49c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridEnvironment()\n",
    "\n",
    "terminated, truncated = False, False\n",
    "observation, info = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd3ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table, rewards_all_episodes = q_learning(env)\n",
    "plt.plot(rewards_all_episodes)\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f963bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1  # down:0, up:1, right:2, left:3\n",
    "obs,info = env.reset()\n",
    "index_lookup_table = np.arange(72).reshape((2,6,6))\n",
    "def get_state_index(obs):\n",
    "    \n",
    "    # print(type(obs))\n",
    "    return index_lookup_table[obs[0],obs[1],obs[2]]\n",
    "state_idx = get_state_index(obs)\n",
    "for step in range(50):\n",
    "\n",
    "    action = np.argmax(Q_table[state_idx, :])\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    new_state_idx = get_state_index(obs)\n",
    "    \n",
    "    state_idx = new_state_idx\n",
    "\n",
    "    env.render()\n",
    "    \n",
    "    if terminated:# or truncated:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd085a5-e1a9-4595-977a-273dbc305710",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 0  #  down:0, up:1, right:2, left:3\n",
    "observation, reward, done, truncated, info = env.step(action)\n",
    "print(observation)\n",
    "# print(env.agent_pos,env.agent_carry_obj)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0d667-9b4b-402e-a340-5fff0e1cb67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 2  # down:0, up:1, right:2, left:3\n",
    "observation, reward, done, truncated, info = env.step(action)\n",
    "print(observation)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1ea36-7a2a-4c3e-aad2-d46f031fc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 3  # down:0, up:1, right:2, left:3\n",
    "observation, reward, done, truncated, info = env.step(action)\n",
    "print(observation)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a71c2e98-5a09-4d63-9019-899496e0db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition an agent that takes action randomly\n",
    "class RandomAgent:\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.stochasticity = 0.9  # TODO\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "\n",
    "    def step(self, obs):\n",
    "        \"\"\"Takes a step in the environment by choosing an action randomly.\n",
    "\n",
    "        Args:\n",
    "            obs: The current observation.\n",
    "\n",
    "        Returns:\n",
    "            The action to take.\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096d77d-bbfc-4559-ab95-0c0d54ec0c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code for environment and agent implementation. Also shows\n",
    "# visualization of the random agent's movement across the grid. The yellow cell\n",
    "# shows the movement of the agent.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = GridEnvironment()\n",
    "    random_agent = RandomAgent(env)\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "\n",
    "    while not terminated:  # one episode:\n",
    "\n",
    "        action = random_agent.step(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        print('Action:', action, ', Reward:', reward, ', Done:', terminated)\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
