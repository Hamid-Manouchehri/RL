{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d386e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n*******************************************************************************\\n\\nProject: RL, assignment 1\\nFile: warehouse_robot_env.ipynb\\nAuthor: Hamid Manouchehri\\nEmail: hmanouch@buffalo.edu\\nDate: Feb 7, 2025\\n\\nDescription:\\nCheckpoint 1, preparing an RL environment\\nEnvironmet Scenario: Warehouse Robot\\n\\nLicense:\\nThis script is licensed under the MIT License.\\nYou may obtain a copy of the License at\\n    https://opensource.org/licenses/MIT\\n\\nSPDX-License-Identifier: MIT\\n\\nDisclaimer:\\nThis software is provided \"as is\", without warranty of any kind, express or\\nimplied, including but not limited to the warranties of merchantability,\\nfitness for a particular purpose, and noninfringement. In no event shall the\\nauthors be liable for any claim, damages, or other liability, whether in an\\naction of contract, tort, or otherwise, arising from, out of, or in connection\\nwith the software or the use or other dealings in the software.\\n\\n*******************************************************************************\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "*******************************************************************************\n",
    "\n",
    "Project: RL, assignment 1\n",
    "File: warehouse_robot_Q_learning.ipynb\n",
    "Author: Hamid Manouchehri\n",
    "Email: hmanouch@buffalo.edu\n",
    "Date: Feb 7, 2025\n",
    "\n",
    "Description:\n",
    "Checkpoint 1, preparing an RL environment\n",
    "Environmet Scenario: Warehouse Robot\n",
    "\n",
    "License:\n",
    "This script is licensed under the MIT License.\n",
    "You may obtain a copy of the License at\n",
    "    https://opensource.org/licenses/MIT\n",
    "\n",
    "SPDX-License-Identifier: MIT\n",
    "\n",
    "Disclaimer:\n",
    "This software is provided \"as is\", without warranty of any kind, express or\n",
    "implied, including but not limited to the warranties of merchantability,\n",
    "fitness for a particular purpose, and noninfringement. In no event shall the\n",
    "authors be liable for any claim, damages, or other liability, whether in an\n",
    "action of contract, tort, or otherwise, arising from, out of, or in connection\n",
    "with the software or the use or other dealings in the software.\n",
    "\n",
    "*******************************************************************************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a9f732-125d-43a5-b84b-9a0d4b5b5973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f818e407-cfa9-46ca-9bdc-99aa0d2aea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.colors as mcolors\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1106490-e2d4-4026-ba0f-7f743df10353",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 6\n",
    "grid = np.ones((grid_size, grid_size, 3))\n",
    "# Define grid size\n",
    "grid_size = 6  # 6x6 grid\n",
    "\n",
    "# Initialize grid with default background (White = 4)\n",
    "grid = np.full((grid_size, grid_size), 1, dtype=int)  # Default to background\n",
    "\n",
    "# Define agent and goal positions\n",
    "init_agent_pos = [0, 0]  # Start position\n",
    "goal_pos = [5, 5]  # Goal position\n",
    "\n",
    "# Define integer-based color mapping\n",
    "rgb_colors = {\n",
    "    \"black\": 0,   # Agent\n",
    "    \"white\": 1,   # Goal\n",
    "    \"red\": 2,     # Trap\n",
    "    \"blue\": 3,    # Water\n",
    "    \"green\": 4,   # Background\n",
    "    \"gray\": 5     # Obstacle\n",
    "}\n",
    "\n",
    "# Define color list for visualization\n",
    "color_map = {\n",
    "    0: \"black\",\n",
    "    1: \"white\",\n",
    "    2: \"red\",\n",
    "    3: \"blue\",\n",
    "    4: \"green\",\n",
    "    5: \"gray\"\n",
    "}\n",
    "\n",
    "agent_obj_state = {\n",
    "    0: \"no_obj\",\n",
    "    1: \"have_obj\"\n",
    "}\n",
    "\n",
    "# Assign integer values to the grid\n",
    "grid[tuple(init_agent_pos)] = rgb_colors[\"black\"]  # Agent\n",
    "grid[tuple(goal_pos)] = rgb_colors[\"green\"]  # Goal\n",
    "\n",
    "# Create a colormap using actual colors\n",
    "cmap = mcolors.ListedColormap([color_map[i] for i in range(len(color_map))])\n",
    "\n",
    "for i in range(6):  # rows\n",
    "    for j in range(6):  # columns\n",
    "        text = \"\"\n",
    "        if [i, j] == init_agent_pos:\n",
    "            text = \"Agent\"\n",
    "        elif [i, j] == goal_pos:\n",
    "            text = \"Goal\"\n",
    "        \n",
    "        # Only annotate if there is text to display\n",
    "        if text:\n",
    "            plt.text(j, i, text, ha=\"center\", va=\"center\", color=\"white\", fontsize=8)\n",
    "\n",
    "\n",
    "# Display the grid with correct colors\n",
    "plt.imshow(grid, cmap=cmap, vmin=0, vmax=len(color_map) - 1)\n",
    "plt.title(\"Grid Environment\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ebcf2b4-c920-4a97-9878-5299967e9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the Grid Environment class.\n",
    "\n",
    "class GridEnvironment(gym.Env):\n",
    "    # Attribute of a Gym class that provides info about the render modes\n",
    "    metadata = { 'render.modes': [] }\n",
    "\n",
    "    # Initialization function\n",
    "    def __init__(self):\n",
    "        # Initializes the class\n",
    "        # Define action and observation space\n",
    "        self.observation_space = spaces.Discrete(36)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.max_timesteps = 500  # TODO\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.timestep = 0\n",
    "        self.agent_carry_obj = False\n",
    "        self.agent_pos = [0,0]\n",
    "        self.goal_pos = [5,5]\n",
    "        self.object_pos = [1, 2]\n",
    "        self.obstacle_1_pos = [2, 2]\n",
    "\n",
    "        self.state = np.ones((6,6))\n",
    "        self.state[tuple(self.agent_pos)] = rgb_colors[\"black\"]\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"green\"]\n",
    "        self.state[tuple(self.object_pos)] = rgb_colors[\"blue\"]\n",
    "        self.state[tuple(self.obstacle_1_pos)] = rgb_colors[\"red\"]\n",
    "\n",
    "\n",
    "    # Reset function\n",
    "    def reset(self, **kwargs):\n",
    "        self.agent_carry_obj = False\n",
    "        self.agent_pos = [0,0]\n",
    "        self.goal_pos = [5,5]\n",
    "        self.object_pos = [1, 2]\n",
    "        self.obstacle_1_pos = [2, 2]\n",
    "        self.reward = 0\n",
    "        self.timestep = 0\n",
    "        self.state = np.ones((6,6))\n",
    "        self.state[tuple(self.agent_pos)] = rgb_colors[\"black\"]\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"green\"]\n",
    "        self.state[tuple(self.object_pos)] = rgb_colors[\"blue\"]\n",
    "        self.state[tuple(self.obstacle_1_pos)] = rgb_colors[\"red\"]\n",
    "        observation = np.append(int(self.agent_carry_obj),np.array(self.agent_pos))\n",
    "\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        return observation, info\n",
    "\n",
    "\n",
    "    def pick_up_obj(self):\n",
    "        self.state[tuple(self.object_pos)] = rgb_colors[\"white\"]\n",
    "        self.agent_carry_obj = True\n",
    "\n",
    "\n",
    "    def drop_off_obj(self):\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"white\"]\n",
    "        self.agent_carry_obj = False\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        self.reward -= 1\n",
    "        terminated = False\n",
    "\n",
    "        # Compute the potential new position based on the action.\n",
    "        new_agent_pos = self.agent_pos.copy()\n",
    "        if action == 0:  # down\n",
    "            new_agent_pos[0] += 1\n",
    "        elif action == 1:  # up\n",
    "            new_agent_pos[0] -= 1\n",
    "        elif action == 2:  # right\n",
    "            new_agent_pos[1] += 1\n",
    "        elif action == 3:  # left\n",
    "            new_agent_pos[1] -= 1\n",
    "\n",
    "        # Ensure the new position is within bounds.\n",
    "        new_agent_pos = np.clip(new_agent_pos, 0, 5).tolist()\n",
    "\n",
    "        # Obstacle avoidance: If the new position is the obstacle, do not update agent_pos.\n",
    "        if np.array_equal(new_agent_pos, self.obstacle_1_pos):\n",
    "            self.reward -= 25  # Apply penalty\n",
    "            # Optionally, you might decide to leave the agent in place:\n",
    "            # new_agent_pos remains as the current position.\n",
    "        else:\n",
    "            # Otherwise, update the agent's position.\n",
    "            self.agent_pos = new_agent_pos\n",
    "\n",
    "        # Update the grid state.\n",
    "        self.state = np.ones((6,6))\n",
    "        self.state[tuple(self.agent_pos)] = rgb_colors[\"black\"]\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"green\"]\n",
    "        if not self.agent_carry_obj:\n",
    "            self.state[tuple(self.object_pos)] = rgb_colors[\"blue\"]\n",
    "        self.state[tuple(self.obstacle_1_pos)] = rgb_colors[\"red\"]\n",
    "\n",
    "        # Create the observation.\n",
    "        observation = np.append(int(self.agent_carry_obj), np.array(self.agent_pos))\n",
    "\n",
    "        # Check for picking up the object.\n",
    "        if np.array_equal(self.agent_pos, self.object_pos) and (self.agent_carry_obj == False):\n",
    "            self.pick_up_obj()\n",
    "            self.reward += 25\n",
    "            self.state[tuple(self.object_pos)] = rgb_colors[\"white\"]\n",
    "\n",
    "        # Check for dropping off the object.\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos) and (self.agent_carry_obj == True):\n",
    "            self.drop_off_obj()\n",
    "            self.reward += 100\n",
    "            terminated = True\n",
    "            self.state[tuple(self.goal_pos)] = rgb_colors[\"black\"]\n",
    "\n",
    "        # Termination condition based on timestep.\n",
    "        self.timestep += 1\n",
    "        if self.timestep >= self.max_timesteps:\n",
    "            terminated = True\n",
    "\n",
    "        # Check if agent remains within permitted cells.\n",
    "        if np.all((np.asarray(self.agent_pos) >= 0) & (np.asarray(self.agent_pos) <= 5)):\n",
    "            truncated = True\n",
    "        else:\n",
    "            truncated = False\n",
    "\n",
    "        info = {}\n",
    "        return observation, self.reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "\n",
    "    # Render function: Visualizes the environment\n",
    "    def render(self):\n",
    "\n",
    "        cmap = mcolors.ListedColormap([color_map[i] for i in range(len(color_map))])\n",
    "        plt.imshow(self.state, cmap=cmap, vmin=0, vmax=len(color_map) - 1)\n",
    "        \n",
    "        # Annotate the grid with text labels\n",
    "        for i in range(6):  # rows\n",
    "            for j in range(6):  # columns\n",
    "                label = \"\"\n",
    "                if np.array_equal([i, j], self.agent_pos):\n",
    "                    label = \"Agent\"\n",
    "                elif np.array_equal([i, j], self.goal_pos):\n",
    "                    label = \"Goal\"\n",
    "                elif np.array_equal([i, j], self.object_pos):\n",
    "                    label = \"Obj\"\n",
    "                elif np.array_equal([i, j], self.obstacle_1_pos):\n",
    "                    label = \"Obs\"\n",
    "                \n",
    "                if label:\n",
    "                    plt.text(j, i, label, ha=\"center\", va=\"center\", color=\"white\", fontsize=8)\n",
    "        \n",
    "        plt.title(\"Grid Environment\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d7c8793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_index(obs, index_lookup_table):\n",
    "            \n",
    "    return index_lookup_table[obs[0],obs[1],obs[2]]\n",
    "\n",
    "\n",
    "def q_learning(env, num_episodes=1000, max_steps=100, alpha=0.1, gamma=0.99,\n",
    "               epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.995,det=0):\n",
    "        \"\"\"\n",
    "        Performs Q-learning on a given environment.\n",
    "        \n",
    "        Parameters:\n",
    "        env: The environment instance (should have reset, step, and get_state_index-like behavior)\n",
    "        num_episodes: Number of episodes for training.\n",
    "        max_steps: Maximum steps per episode.\n",
    "        alpha: Learning rate.\n",
    "        gamma: Discount factor.\n",
    "        epsilon: Initial exploration rate.\n",
    "        epsilon_min: Minimum exploration rate.\n",
    "        epsilon_decay: Decay factor for epsilon after each episode.\n",
    "        \n",
    "        Returns:\n",
    "        Q: The trained Q-table.\n",
    "        rewards_all_episodes: List with total rewards per episode.\n",
    "        \"\"\"\n",
    "        index_lookup_table = np.arange(72).reshape((2,6,6))\n",
    "        grid_size = 6\n",
    "        num_states = grid_size * grid_size\n",
    "        num_actions = env.action_space.n  # 4 actions\n",
    "        \n",
    "        # Initialize Q-table with zeros.\n",
    "        Q_table = np.zeros((num_states*2, num_actions))\n",
    "        rewards_all_episodes = []   \n",
    "        epsilon_arr = []         \n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            obs, info = env.reset()\n",
    "            state_idx = get_state_index(obs, index_lookup_table)\n",
    "            total_rewards = 0\n",
    "            for step in range(max_steps):\n",
    "                # Epsilon-greedy action selection.\n",
    "                \n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(Q_table[state_idx, :])\n",
    "                \n",
    "                if np.random.rand() < det:# Stochastic\n",
    "                    reward=-1\n",
    "                else:\n",
    "                    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "                # print(\"agent location: \", env.agent_pos, \"action\", action)\n",
    "                \n",
    "                total_rewards += reward\n",
    "                new_state_idx = get_state_index(obs, index_lookup_table)\n",
    "                \n",
    "                # Q-learning update rule.\n",
    "                Q_table[state_idx, action] += alpha * (\n",
    "                    reward + gamma * np.max(Q_table[new_state_idx, :]) - Q_table[state_idx, action]\n",
    "                )\n",
    "                \n",
    "                state_idx = new_state_idx\n",
    "                \n",
    "                if terminated:# or truncated:\n",
    "                    break\n",
    "            \n",
    "            # Decay epsilon after each episode.\n",
    "            epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "            rewards_all_episodes.append(total_rewards)\n",
    "            epsilon_arr.append(epsilon)\n",
    "        \n",
    "        return Q_table, rewards_all_episodes, epsilon_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485161be-db60-41f5-af64-77d69f49c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## Init environment & Reset ##\n",
    "##############################\n",
    "env = GridEnvironment()\n",
    "terminated, truncated = False, False\n",
    "observation, info = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10e134fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(Q_table, num_episodes=10, max_steps=100, det=0):\n",
    "\n",
    "    \n",
    "    index_lookup_table = np.arange(72).reshape((2,6,6))\n",
    "    rewards_all_episodes = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        terminated = 0\n",
    "        obs, info = env.reset()\n",
    "        state_idx = get_state_index(obs, index_lookup_table)\n",
    "        total_rewards = 0\n",
    "        for step in range(max_steps):\n",
    "            \n",
    "            action = np.argmax(Q_table[state_idx, :])\n",
    "            \n",
    "            if np.random.rand() < det:  # Stochastic\n",
    "                reward=-1\n",
    "            else:\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # print(\"agent location: \", env.agent_pos, \"action\", action)\n",
    "            \n",
    "            total_rewards += reward\n",
    "            new_state_idx = get_state_index(obs, index_lookup_table)\n",
    "            \n",
    "            \n",
    "            state_idx = new_state_idx\n",
    "            \n",
    "            if terminated:# or truncated:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon after each episode.\n",
    "        rewards_all_episodes.append(total_rewards)\n",
    "\n",
    "    \n",
    "    return  rewards_all_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd3ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#### Q-Learning Training #####\n",
    "##############################\n",
    "det=0.1\n",
    "Q_table, rewards_all_episodes, epsilon_arr = q_learning(env, det=det)\n",
    "plt.plot(rewards_all_episodes)\n",
    "print(Q_table)\n",
    "plt.title('training agent')\n",
    "\n",
    "pkl_file_name = 'Q_table_stochastic.pkl'\n",
    "dir_to_save_pkl_file = \"/home/hamid/UB/courses/sp2025/CSE546_RL/RL/scripts/data/\"\n",
    "with open(dir_to_save_pkl_file + pkl_file_name, 'wb') as f:\n",
    "    pickle.dump(Q_table, f)\n",
    "\n",
    "# with open(dir_to_save_pkl_file + pkl_file_name, 'rb') as f:\n",
    "#     Q_table = pickle.load(f)\n",
    "\n",
    "plt.show()\n",
    "t=test(Q_table, num_episodes=10, max_steps=100, det=det)\n",
    "plt.plot(t)\n",
    "plt.title('test agent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f16386",
   "metadata": {},
   "outputs": [],
   "source": [
    "det=0.\n",
    "Q_table, rewards_all_episodes, epsilon_arr = q_learning(env, det=det)\n",
    "plt.plot(rewards_all_episodes)\n",
    "print(Q_table)\n",
    "plt.title('training agent')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epsilon_arr)\n",
    "\n",
    "plt.title('epsilon decay')\n",
    "\n",
    "pkl_file_name = 'Q_table_deterministic.pkl'\n",
    "dir_to_save_pkl_file = \"/home/hamid/UB/courses/sp2025/CSE546_RL/RL/scripts/data/\"\n",
    "with open(dir_to_save_pkl_file + pkl_file_name, 'wb') as f:\n",
    "    pickle.dump(Q_table, f)\n",
    "\n",
    "# with open(dir_to_save_pkl_file + pkl_file_name, 'rb') as f:\n",
    "#     Q_table = pickle.load(f)\n",
    "\n",
    "plt.show()\n",
    "t=test(Q_table, num_episodes=10, max_steps=100, det=det)\n",
    "plt.plot(t)\n",
    "plt.title('test agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070cf260",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=test(Q_table, num_episodes=10, max_steps=100, det=0.2)\n",
    "plt.plot(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f963bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "## Single Episode of Testing The Agent ##\n",
    "#########################################\n",
    "\n",
    "observation, info = env.reset()\n",
    "env.render()\n",
    "index_lookup_table = np.arange(72).reshape((2,6,6))\n",
    "def get_state_index(obs):\n",
    "    return index_lookup_table[obs[0], obs[1], obs[2]]\n",
    "\n",
    "state_idx = get_state_index(observation)\n",
    "\n",
    "for step in range(50):\n",
    "    action = np.argmax(Q_table[state_idx, :])\n",
    "    \n",
    "    if np.random.rand() < det:  # stochastic\n",
    "        reward = -1\n",
    "    else:\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    new_state_idx = get_state_index(obs)\n",
    "    state_idx = new_state_idx\n",
    "    \n",
    "    env.render()\n",
    "    time.sleep(.1)  # TODO Pause to see the update\n",
    "    \n",
    "    clear_output(wait=True)  # Clear the previous output so the next plot replaces it\n",
    "    \n",
    "    if terminated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_idx = get_state_index(observation)\n",
    "\n",
    "for step in range(50):\n",
    "    action = np.argmax(Q_table[state_idx, :])\n",
    "    \n",
    "    if np.random.rand() < det:  # stochastic\n",
    "        reward = -1\n",
    "    else:\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    new_state_idx = get_state_index(obs)\n",
    "    state_idx = new_state_idx\n",
    "    \n",
    "    env.render()\n",
    "    time.sleep(.1)  # TODO Pause to see the update\n",
    "    \n",
    "    clear_output(wait=True)  # Clear the previous output so the next plot replaces it\n",
    "    \n",
    "    if terminated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a71c2e98-5a09-4d63-9019-899496e0db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "########## Random Agent Class ###########\n",
    "#########################################\n",
    "\n",
    "class RandomAgent:\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "\n",
    "    def step(self, obs):\n",
    "        \"\"\"Takes a step in the environment by choosing an action randomly.\n",
    "\n",
    "        Args:\n",
    "            obs: The current observation.\n",
    "\n",
    "        Returns:\n",
    "            The action to take.\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096d77d-bbfc-4559-ab95-0c0d54ec0c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code for environment and agent implementation. Also shows\n",
    "# visualization of the random agent's movement across the grid. The yellow cell\n",
    "# shows the movement of the agent.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = GridEnvironment()\n",
    "    random_agent = RandomAgent(env)\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "\n",
    "    while not terminated:  # one episode:\n",
    "\n",
    "        action = random_agent.step(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        # print('Action:', action, ', Reward:', reward, ', Done:', terminated)\n",
    "        \n",
    "        env.render()\n",
    "        time.sleep(.05)  # TODO Pause to see the update\n",
    "        \n",
    "        clear_output(wait=True)  # Clear the previous output so the next plot replaces it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
