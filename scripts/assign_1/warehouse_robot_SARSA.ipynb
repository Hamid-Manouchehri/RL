{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "*******************************************************************************\n",
    "\n",
    "Project: RL, assignment 1\n",
    "File: warehouse_robot_SARSA.ipynb\n",
    "Author: Hamid Manouchehri\n",
    "Email: hmanouch@buffalo.edu\n",
    "Date: Feb 24, 2025\n",
    "\n",
    "Description:\n",
    "Environmet Scenario: Warehouse Robot\n",
    "\n",
    "License:\n",
    "This script is licensed under the MIT License.\n",
    "You may obtain a copy of the License at\n",
    "    https://opensource.org/licenses/MIT\n",
    "\n",
    "SPDX-License-Identifier: MIT\n",
    "\n",
    "Disclaimer:\n",
    "This software is provided \"as is\", without warranty of any kind, express or\n",
    "implied, including but not limited to the warranties of merchantability,\n",
    "fitness for a particular purpose, and noninfringement. In no event shall the\n",
    "authors be liable for any claim, damages, or other liability, whether in an\n",
    "action of contract, tort, or otherwise, arising from, out of, or in connection\n",
    "with the software or the use or other dealings in the software.\n",
    "\n",
    "*******************************************************************************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.colors as mcolors\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 6\n",
    "grid = np.ones((grid_size, grid_size, 3))\n",
    "# Define grid size\n",
    "grid_size = 6  # 6x6 grid\n",
    "\n",
    "# Initialize grid with default background (White = 4)\n",
    "grid = np.full((grid_size, grid_size), 1, dtype=int)  # Default to background\n",
    "\n",
    "# Define agent and goal positions\n",
    "init_agent_pos = [0, 0]  # Start position\n",
    "goal_pos = [5, 5]  # Goal position\n",
    "\n",
    "# Define integer-based color mapping\n",
    "rgb_colors = {\n",
    "    \"black\": 0,   # Agent\n",
    "    \"white\": 1,   # Goal\n",
    "    \"red\": 2,     # Trap\n",
    "    \"blue\": 3,    # Water\n",
    "    \"green\": 4,   # Background\n",
    "    \"gray\": 5     # Obstacle\n",
    "}\n",
    "\n",
    "# Define color list for visualization\n",
    "color_map = {\n",
    "    0: \"black\",\n",
    "    1: \"white\",\n",
    "    2: \"red\",\n",
    "    3: \"blue\",\n",
    "    4: \"green\",\n",
    "    5: \"gray\"\n",
    "}\n",
    "\n",
    "agent_obj_state = {\n",
    "    0: \"no_obj\",\n",
    "    1: \"have_obj\"\n",
    "}\n",
    "\n",
    "# Assign integer values to the grid\n",
    "grid[tuple(init_agent_pos)] = rgb_colors[\"black\"]  # Agent\n",
    "grid[tuple(goal_pos)] = rgb_colors[\"green\"]  # Goal\n",
    "\n",
    "# Create a colormap using actual colors\n",
    "cmap = mcolors.ListedColormap([color_map[i] for i in range(len(color_map))])\n",
    "\n",
    "for i in range(6):  # rows\n",
    "    for j in range(6):  # columns\n",
    "        text = \"\"\n",
    "        if [i, j] == init_agent_pos:\n",
    "            text = \"Agent\"\n",
    "        elif [i, j] == goal_pos:\n",
    "            text = \"Goal\"\n",
    "        \n",
    "        # Only annotate if there is text to display\n",
    "        if text:\n",
    "            plt.text(j, i, text, ha=\"center\", va=\"center\", color=\"white\", fontsize=8)\n",
    "\n",
    "\n",
    "# Display the grid with correct colors\n",
    "plt.imshow(grid, cmap=cmap, vmin=0, vmax=len(color_map) - 1)\n",
    "plt.title(\"Grid Environment\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the Grid Environment class.\n",
    "\n",
    "class GridEnvironment(gym.Env):\n",
    "    # Attribute of a Gym class that provides info about the render modes\n",
    "    metadata = { 'render.modes': [] }\n",
    "\n",
    "    # Initialization function\n",
    "    def __init__(self):\n",
    "        # Initializes the class\n",
    "        # Define action and observation space\n",
    "        self.observation_space = spaces.Discrete(36)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.max_timesteps = 500  # TODO\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.timestep = 0\n",
    "        self.agent_carry_obj = False\n",
    "        self.agent_pos = [0,0]\n",
    "        self.goal_pos = [5,5]\n",
    "        self.object_pos = [1, 2]\n",
    "        self.obstacle_1_pos = [2, 2]\n",
    "\n",
    "        self.state = np.ones((6,6))\n",
    "        self.state[tuple(self.agent_pos)] = rgb_colors[\"black\"]\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"green\"]\n",
    "        self.state[tuple(self.object_pos)] = rgb_colors[\"blue\"]\n",
    "        self.state[tuple(self.obstacle_1_pos)] = rgb_colors[\"red\"]\n",
    "\n",
    "\n",
    "    # Reset function\n",
    "    def reset(self, **kwargs):\n",
    "        self.agent_carry_obj = False\n",
    "        self.agent_pos = [0,0]\n",
    "        self.goal_pos = [5,5]\n",
    "        self.object_pos = [1, 2]\n",
    "        self.obstacle_1_pos = [2, 2]\n",
    "        self.reward = 0\n",
    "        self.timestep = 0\n",
    "        self.state = np.ones((6,6))\n",
    "        self.state[tuple(self.agent_pos)] = rgb_colors[\"black\"]\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"green\"]\n",
    "        self.state[tuple(self.object_pos)] = rgb_colors[\"blue\"]\n",
    "        self.state[tuple(self.obstacle_1_pos)] = rgb_colors[\"red\"]\n",
    "        observation = np.append(int(self.agent_carry_obj),np.array(self.agent_pos))\n",
    "\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        return observation, info\n",
    "\n",
    "\n",
    "    def pick_up_obj(self):\n",
    "        self.state[tuple(self.object_pos)] = rgb_colors[\"white\"]\n",
    "        self.agent_carry_obj = True\n",
    "\n",
    "\n",
    "    def drop_off_obj(self):\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"white\"]\n",
    "        self.agent_carry_obj = False\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        self.reward -= 1\n",
    "        terminated = False\n",
    "\n",
    "        # Compute the potential new position based on the action.\n",
    "        new_agent_pos = self.agent_pos.copy()\n",
    "        if action == 0:  # down\n",
    "            new_agent_pos[0] += 1\n",
    "        elif action == 1:  # up\n",
    "            new_agent_pos[0] -= 1\n",
    "        elif action == 2:  # right\n",
    "            new_agent_pos[1] += 1\n",
    "        elif action == 3:  # left\n",
    "            new_agent_pos[1] -= 1\n",
    "\n",
    "        # Ensure the new position is within bounds.\n",
    "        new_agent_pos = np.clip(new_agent_pos, 0, 5).tolist()\n",
    "\n",
    "        # Obstacle avoidance: If the new position is the obstacle, do not update agent_pos.\n",
    "        if np.array_equal(new_agent_pos, self.obstacle_1_pos):\n",
    "            self.reward -= 25  # Apply penalty\n",
    "            # Optionally, you might decide to leave the agent in place:\n",
    "            # new_agent_pos remains as the current position.\n",
    "        else:\n",
    "            # Otherwise, update the agent's position.\n",
    "            self.agent_pos = new_agent_pos\n",
    "\n",
    "        # Update the grid state.\n",
    "        self.state = np.ones((6,6))\n",
    "        self.state[tuple(self.agent_pos)] = rgb_colors[\"black\"]\n",
    "        self.state[tuple(self.goal_pos)] = rgb_colors[\"green\"]\n",
    "        if not self.agent_carry_obj:\n",
    "            self.state[tuple(self.object_pos)] = rgb_colors[\"blue\"]\n",
    "        self.state[tuple(self.obstacle_1_pos)] = rgb_colors[\"red\"]\n",
    "\n",
    "        # Create the observation.\n",
    "        observation = np.append(int(self.agent_carry_obj), np.array(self.agent_pos))\n",
    "\n",
    "        # Check for picking up the object.\n",
    "        if np.array_equal(self.agent_pos, self.object_pos) and (self.agent_carry_obj == False):\n",
    "            self.pick_up_obj()\n",
    "            self.reward += 25\n",
    "            self.state[tuple(self.object_pos)] = rgb_colors[\"white\"]\n",
    "\n",
    "        # Check for dropping off the object.\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos) and (self.agent_carry_obj == True):\n",
    "            self.drop_off_obj()\n",
    "            self.reward += 100\n",
    "            terminated = True\n",
    "            self.state[tuple(self.goal_pos)] = rgb_colors[\"black\"]\n",
    "\n",
    "        # Termination condition based on timestep.\n",
    "        self.timestep += 1\n",
    "        if self.timestep >= self.max_timesteps:\n",
    "            terminated = True\n",
    "\n",
    "        # Check if agent remains within permitted cells.\n",
    "        if np.all((np.asarray(self.agent_pos) >= 0) & (np.asarray(self.agent_pos) <= 5)):\n",
    "            truncated = True\n",
    "        else:\n",
    "            truncated = False\n",
    "\n",
    "        info = {}\n",
    "        return observation, self.reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "\n",
    "    # Render function: Visualizes the environment\n",
    "    def render(self):\n",
    "\n",
    "        cmap = mcolors.ListedColormap([color_map[i] for i in range(len(color_map))])\n",
    "        plt.imshow(self.state, cmap=cmap, vmin=0, vmax=len(color_map) - 1)\n",
    "        \n",
    "        # Annotate the grid with text labels\n",
    "        for i in range(6):  # rows\n",
    "            for j in range(6):  # columns\n",
    "                label = \"\"\n",
    "                if np.array_equal([i, j], self.agent_pos):\n",
    "                    label = \"Agent\"\n",
    "                elif np.array_equal([i, j], self.goal_pos):\n",
    "                    label = \"Goal\"\n",
    "                elif np.array_equal([i, j], self.object_pos):\n",
    "                    label = \"Obj\"\n",
    "                elif np.array_equal([i, j], self.obstacle_1_pos):\n",
    "                    label = \"Obs\"\n",
    "                \n",
    "                if label:\n",
    "                    plt.text(j, i, label, ha=\"center\", va=\"center\", color=\"white\", fontsize=8)\n",
    "        \n",
    "        plt.title(\"Grid Environment\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## Init environment & Reset ##\n",
    "##############################\n",
    "env = GridEnvironment()\n",
    "terminated, truncated = False, False\n",
    "observation, info = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA(env, num_episodes=1000, max_steps=100, alpha=0.1, gamma=0.99,\n",
    "               epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.995):\n",
    "        \"\"\"\n",
    "        Performs SARSA on a given environment.\n",
    "        \n",
    "        Parameters:\n",
    "        env: The environment instance (should have reset, step, and get_state_index-like behavior)\n",
    "        num_episodes: Number of episodes for training.\n",
    "        max_steps: Maximum steps per episode.\n",
    "        alpha: Learning rate.\n",
    "        gamma: Discount factor.\n",
    "        epsilon: Initial exploration rate.\n",
    "        epsilon_min: Minimum exploration rate.\n",
    "        epsilon_decay: Decay factor for epsilon after each episode.\n",
    "        \n",
    "        Returns:\n",
    "        Q: The trained Q-table.\n",
    "        rewards_all_episodes: List with total rewards per episode.\n",
    "        \"\"\"\n",
    "        index_lookup_table = np.arange(72).reshape((2,6,6))\n",
    "        grid_size = 6\n",
    "        num_states = grid_size * grid_size\n",
    "        num_actions = env.action_space.n  # 4 actions\n",
    "        \n",
    "        # Initialize Q-table with zeros.\n",
    "        Q_table = np.zeros((num_states*2, num_actions))\n",
    "        rewards_all_episodes = []\n",
    "        \n",
    "        def get_state_index(obs):\n",
    "            \n",
    "            # print(type(obs))\n",
    "            return index_lookup_table[obs[0],obs[1],obs[2]]\n",
    "            \n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            obs, info = env.reset()\n",
    "            state_idx = get_state_index(obs)\n",
    "            total_rewards = 0\n",
    "            for step in range(max_steps):\n",
    "                # Epsilon-greedy action selection.\n",
    "                \n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(Q_table[state_idx, :])\n",
    "                \n",
    "                if np.random.rand() < 0:# Stochastic\n",
    "                    reward=-1\n",
    "                    print('wakka wakka whatever')\n",
    "                else:\n",
    "                    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "                print(\"agent location: \", env.agent_pos, \"action\", action)\n",
    "                \n",
    "                total_rewards += reward\n",
    "                new_state_idx = get_state_index(obs)\n",
    "                \n",
    "                # Q-learning update rule.\n",
    "                Q_table[state_idx, action] += alpha * (\n",
    "                    reward + gamma * np.max(Q_table[new_state_idx, :]) - Q_table[state_idx, action]\n",
    "                )\n",
    "                \n",
    "                state_idx = new_state_idx\n",
    "                \n",
    "                if terminated:# or truncated:\n",
    "                    break\n",
    "            \n",
    "            # Decay epsilon after each episode.\n",
    "            epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "            rewards_all_episodes.append(total_rewards)\n",
    "        \n",
    "        return Q_table, rewards_all_episodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
