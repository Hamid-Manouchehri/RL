{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4d8c9b006ada45",
   "metadata": {},
   "source": [
    "## <center>CSE 546: Reinforcement Learning</center>\n",
    "### <center>Prof. Alina Vereshchaka</center>\n",
    "#### <center>Spring 2025</center>\n",
    "\n",
    "Welcome to the Assignment 3, Part 1: Introduction to Actor-Critic Methods! It includes the implementation of simple actor and critic networks and best practices used in modern Actor-Critic algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a6d891e2fb312",
   "metadata": {},
   "source": [
    "## Section 0: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beb6c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "# used in Jupyter to install gymnasium dependencies\n",
    "#!pip install swig\n",
    "#!pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53473293aa9daf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10f5404f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d9c34ff222994",
   "metadata": {},
   "source": [
    "## Section 1: Actor-Critic Network Architectures and Loss Computation\n",
    "\n",
    "In this section, you will explore two common architectural designs for Actor-Critic methods and implement their corresponding loss functions using dummy tensors. These architectures are:\n",
    "- A. Completely separate actor and critic networks\n",
    "- B. A shared network with two output heads\n",
    "\n",
    "Both designs are widely used in practice. Shared networks are often more efficient and generalize better, while separate networks offer more control and flexibility.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971fa7887dd4f858",
   "metadata": {},
   "source": [
    "### Task 1a â€“ Separate Actor and Critic Networks with Loss Function\n",
    "\n",
    "Define a class `SeparateActorCritic`. Your goal is to:\n",
    "- Create two completely independent neural networks: one for the actor and one for the critic.\n",
    "- The actor should output a probability distribution over discrete actions (use `nn.Softmax`).\n",
    "- The critic should output a single scalar value.\n",
    "\n",
    " Use `nn.ReLU()` as your activation function. Include at least one hidden layer of reasonable width (e.g. 64 or 128 units).\n",
    "\n",
    "```python\n",
    "# TODO: Define SeparateActorCritic class\n",
    "```\n",
    "\n",
    " Next, simulate training using dummy tensors:\n",
    "1. Generate dummy tensors for log-probabilities, returns, estimated values, and entropies.\n",
    "2. Compute the actor loss using the advantage (return - value).\n",
    "3. Compute the critic loss as mean squared error between values and returns.\n",
    "4. Use a single optimizer for both the Actor and the Critic. In this case, combine the actor and critic losses into a total loss and perform backpropagation.\n",
    "5. Use a separate optimizers for both the Actor and the Critic. In this case, keep the actor and critic losses separate and perform backpropagation.\n",
    "\n",
    "```python\n",
    "# TODO: Simulate loss computation and backpropagation\n",
    "```\n",
    "\n",
    "ðŸ”— Helpful references:\n",
    "- PyTorch Softmax: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
    "- PyTorch MSE Loss: https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd6b81ed1791e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor loss\n",
      "tensor(-2.8464, grad_fn=<NegBackward0>)\n",
      "Critic loss\n",
      "tensor(1.1790, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define a class SeparateActorCritic with separate networks for actor and critic\n",
    "\n",
    "# BEGIN_YOUR_CODE\n",
    "\n",
    "# PyTorch Neural Network tutorials:\n",
    "# https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "# https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html\n",
    "# Learned about nn.Sequential here (linked in Piazza post @580): https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/torch_layers.py\n",
    "\n",
    "# class SeparateActorCritic inherits class nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "class SeparateActorCritic(nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "    # other than self, each parameter represents the dimension of each layer. aka the # of neurons in each layer\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(SeparateActorCritic, self).__init__()\n",
    "\n",
    "        # actor -------------------------------------\n",
    "\n",
    "        # nn.Sequential is handy container: https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # critic --------------------------\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    # forward propagation; \n",
    "    def forward(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        state_value = self.critic(state)\n",
    "        return action_probs, state_value\n",
    "\n",
    "\n",
    "#1 ---\n",
    "batch_size = 100\n",
    "state_dim = 10\n",
    "action_dim = 4\n",
    "hidden_dim = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "actor_critic_instance = SeparateActorCritic(state_dim, action_dim, hidden_dim)\n",
    "\n",
    "states = torch.randn(batch_size, state_dim)\n",
    "actions = torch.randint(0, action_dim, (batch_size,))\n",
    "returns = torch.randn(batch_size)\n",
    "action_prob_results, state_value_results = actor_critic_instance(states)\n",
    "values = state_value_results.squeeze()\n",
    "\n",
    "log_probs = torch.log(action_prob_results)\n",
    "selected_log_probs = log_probs.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "entropies = -torch.sum(action_prob_results * log_probs, dim=1)\n",
    "\n",
    "\n",
    "#2 ---\n",
    "# https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html -  for preventing backprop\n",
    "advantages = returns - state_value_results.detach()\n",
    "actor_loss = -(selected_log_probs * advantages).mean()\n",
    "print(\"Actor loss\")\n",
    "print(actor_loss)\n",
    "\n",
    "#3 ---\n",
    "critic_loss = F.mse_loss(values, returns.detach())\n",
    "entropy_bonus = entropies.mean()\n",
    "print(\"Critic loss\")\n",
    "print(critic_loss)\n",
    "\n",
    "#4 ---\n",
    "# Used Adam optimizer (tip from Piazza post @580) https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "single_optimzer = optimizer = optim.Adam(actor_critic_instance.parameters(), learning_rate)\n",
    "total_loss = actor_loss + critic_loss - 0.01*entropy_bonus\n",
    "single_optimzer.zero_grad()\n",
    "#backpropagation: https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\n",
    "single_optimzer.step()\n",
    "\n",
    "\n",
    "#5 --- two optimizers\n",
    "\n",
    "actor_optimizer = optim.Adam(actor_critic_instance.actor.parameters(), learning_rate)\n",
    "actor_optimizer.zero_grad()\n",
    "actor_loss.backward()\n",
    "actor_optimizer.step()\n",
    "\n",
    "critic_optimizer = optim.Adam(actor_critic_instance.critic.parameters(), learning_rate)\n",
    "critic_optimizer.zero_grad()\n",
    "critic_loss.backward()\n",
    "critic_optimizer.step()\n",
    "\n",
    "\n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e90c88108cd2e",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER: The motivation behind a setup with separate actor and critic neural networks is that when you train them independently, the training results of each the actor and the critic are prioritized at the same time, so there is no compromising one's results for the other in a training cycle. This will in theory make the actor and critic better at their own jobs, and may be preferred if the task is small enough or your resources are high enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64081a606b93029d",
   "metadata": {},
   "source": [
    "### Task 1b â€“ Shared Network with Actor and Critic Heads + Loss Function\n",
    "\n",
    "Now define a class `SharedActorCritic`:\n",
    "- Build a shared base network (e.g., linear layer + ReLU)\n",
    "- Create two heads: one for actor (output action probabilities) and one for critic (output state value)\n",
    "\n",
    "```python\n",
    "# TODO: Define SharedActorCritic class\n",
    "```\n",
    "\n",
    "Then:\n",
    "1. Pass a dummy input tensor through the model to obtain action probabilities and value.\n",
    "2. Simulate dummy rewards and compute advantage.\n",
    "3. Compute the actor and critic losses, combine them, and backpropagate.\n",
    "\n",
    "```python\n",
    "# TODO: Simulate shared network loss computation and backpropagation\n",
    "```\n",
    "\n",
    " Use `nn.Softmax` for actor output and `nn.Linear` for scalar critic output.\n",
    "\n",
    "ðŸ”— More reading:\n",
    "- Policy Gradient Methods: https://spinningup.openai.com/en/latest/algorithms/vpg.html\n",
    "- Actor-Critic Overview: https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial\n",
    "- PyTorch Categorical Distribution: https://pytorch.org/docs/stable/distributions.html#categorical\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a48f882fff11aecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action probabilities:\n",
      "tensor([[0.0194, 0.0130, 0.0115,  ..., 0.0135, 0.0086, 0.0130],\n",
      "        [0.0188, 0.0094, 0.0140,  ..., 0.0108, 0.0109, 0.0136],\n",
      "        [0.0196, 0.0109, 0.0243,  ..., 0.0108, 0.0051, 0.0133],\n",
      "        ...,\n",
      "        [0.0319, 0.0104, 0.0139,  ..., 0.0139, 0.0051, 0.0085],\n",
      "        [0.0231, 0.0119, 0.0114,  ..., 0.0144, 0.0075, 0.0111],\n",
      "        [0.0158, 0.0140, 0.0131,  ..., 0.0117, 0.0099, 0.0159]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "State values:\n",
      "tensor([[0.4887],\n",
      "        [0.4767],\n",
      "        [0.4231],\n",
      "        [0.4230],\n",
      "        [0.4780],\n",
      "        [0.4760],\n",
      "        [0.3652],\n",
      "        [0.4767],\n",
      "        [0.4809],\n",
      "        [0.3440],\n",
      "        [0.4649],\n",
      "        [0.4690],\n",
      "        [0.4559],\n",
      "        [0.4208],\n",
      "        [0.4171],\n",
      "        [0.5588],\n",
      "        [0.4767],\n",
      "        [0.2831],\n",
      "        [0.4276],\n",
      "        [0.3350],\n",
      "        [0.4767],\n",
      "        [0.4767],\n",
      "        [0.5399],\n",
      "        [0.4755],\n",
      "        [0.4767],\n",
      "        [0.5066],\n",
      "        [0.5307],\n",
      "        [0.5739],\n",
      "        [0.4767],\n",
      "        [0.4767],\n",
      "        [0.3374],\n",
      "        [0.2965],\n",
      "        [0.3778],\n",
      "        [0.4310],\n",
      "        [0.4767],\n",
      "        [0.4767],\n",
      "        [0.3990],\n",
      "        [0.4175],\n",
      "        [0.4329],\n",
      "        [0.1320],\n",
      "        [0.1841],\n",
      "        [0.3574],\n",
      "        [0.4269],\n",
      "        [0.4376],\n",
      "        [0.2706],\n",
      "        [0.1906],\n",
      "        [0.4026],\n",
      "        [0.4803],\n",
      "        [0.3886],\n",
      "        [0.4764],\n",
      "        [0.4111],\n",
      "        [0.4917],\n",
      "        [0.3224],\n",
      "        [0.4729],\n",
      "        [0.5461],\n",
      "        [0.3648],\n",
      "        [0.4767],\n",
      "        [0.5172],\n",
      "        [0.4988],\n",
      "        [0.4147],\n",
      "        [0.4069],\n",
      "        [0.5501],\n",
      "        [0.4284],\n",
      "        [0.3738],\n",
      "        [0.3974],\n",
      "        [0.3812],\n",
      "        [0.3678],\n",
      "        [0.4767],\n",
      "        [0.4476],\n",
      "        [0.3624],\n",
      "        [0.5646],\n",
      "        [0.3621],\n",
      "        [0.3726],\n",
      "        [0.5157],\n",
      "        [0.4767],\n",
      "        [0.3778],\n",
      "        [0.4374],\n",
      "        [0.4767],\n",
      "        [0.4767],\n",
      "        [0.4336],\n",
      "        [0.4767],\n",
      "        [0.4677],\n",
      "        [0.3676],\n",
      "        [0.4328],\n",
      "        [0.4619],\n",
      "        [0.5256],\n",
      "        [0.5100],\n",
      "        [0.3686],\n",
      "        [0.4932],\n",
      "        [0.4492],\n",
      "        [0.3698],\n",
      "        [0.4741],\n",
      "        [0.2033],\n",
      "        [0.4740],\n",
      "        [0.5214],\n",
      "        [0.4866],\n",
      "        [0.4767],\n",
      "        [0.4651],\n",
      "        [0.3921],\n",
      "        [0.5513]], grad_fn=<AddmmBackward0>)\n",
      "Advantage:\n",
      "tensor([ 8.3127, 12.8419, 12.7823, 11.5104,  9.5188, 10.8448, 12.2461, 11.1032,\n",
      "         8.0496,  8.8878,  8.1539, 12.3699, 11.3697, 10.7260,  4.5546, 10.3468,\n",
      "        10.3300,  9.8312,  8.9315,  7.2197, 12.9000,  6.1511, 10.3812,  9.2868,\n",
      "        10.4955,  9.5847,  6.4344,  8.1146, 12.6580,  9.3290, 11.6753, 10.8181,\n",
      "        11.5898, 10.9838, 11.0935,  6.7888, 11.0362,  9.4132, 10.5356,  9.7885,\n",
      "         8.6171,  7.6089,  9.5834,  7.5536,  8.9515, 12.9857, 13.8393,  6.9857,\n",
      "        12.9560,  8.4977,  9.9666, 10.1870, 11.4983, 12.2688,  7.8158, 11.4082,\n",
      "        10.4564, 11.2132, 11.0458, 13.3104,  7.0167,  9.9530,  9.1943,  7.3380,\n",
      "        10.0715, 12.8323,  8.1507,  8.9185,  7.4337,  7.6189, 11.2181,  7.2900,\n",
      "         9.4039,  9.2310,  9.8223, 10.2555, 10.0506,  9.4276, 12.3341, 11.4680,\n",
      "        10.2941, 11.0213, 11.2670, 14.5068, 11.8878,  9.6295,  8.6978, 12.4433,\n",
      "         6.9084, 12.2378, 10.5286, 11.2388, 11.7652, 10.6239,  7.4720, 11.7503,\n",
      "        10.6034, 10.9381, 10.7769, 11.5100])\n",
      "Actor loss\n",
      "tensor(14.1615, grad_fn=<NegBackward0>)\n",
      "Critic loss\n",
      "tensor(105.8280, grad_fn=<MseLossBackward0>)\n",
      "Combined loss\n",
      "tensor(119.9758, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_YOUR_CODE\n",
    "\n",
    "class SharedActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(SharedActorCritic, self).__init__()\n",
    "\n",
    "        # base\n",
    "        self.shared_base = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # actor head\n",
    "        self.actor_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # critic head\n",
    "        self.critic_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        shared_features = self.shared_base(state)\n",
    "        action_probs = self.actor_head(shared_features)\n",
    "        state_value = self.critic_head(shared_features)\n",
    "        return action_probs, state_value\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "state_dim = 10\n",
    "action_dim = 4\n",
    "hidden_dim = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "instance = SharedActorCritic(state_dim, action_dim, hidden_dim)\n",
    "optimizer = optim.Adam(instance.parameters(), learning_rate)\n",
    "# dummy data\n",
    "states = torch.randn(batch_size, state_dim)\n",
    "actions = torch.randint(0, action_dim, (batch_size,))\n",
    "returns = torch.randn(batch_size) * 2 + 10\n",
    "\n",
    "action_probs_results, state_values_results = instance(states)\n",
    "# https://pytorch.org/docs/stable/generated/torch.squeeze.html\n",
    "state_values_results = state_values_results.squeeze()\n",
    "print(\"Action probabilities:\")\n",
    "print(action_prob_results)\n",
    "print(\"State values:\")\n",
    "print(state_value_results)\n",
    "\n",
    "log_probs = torch.log(action_probs_results)\n",
    "# https://pytorch.org/docs/stable/generated/torch.gather.html\n",
    "selected_log_probs = log_probs.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "entropies = -torch.sum(action_probs_results * log_probs, dim=1)\n",
    "\n",
    "#3 --- compute advantage, loses\n",
    "advantages = returns - state_values_results.detach()\n",
    "print(\"Advantage:\")\n",
    "print(advantages)\n",
    "actor_loss = -(selected_log_probs * advantages).mean()\n",
    "print(\"Actor loss\")\n",
    "print(actor_loss)\n",
    "critic_loss = F.mse_loss(state_values_results, returns.detach())\n",
    "print(\"Critic loss\")\n",
    "print(critic_loss)\n",
    "entropy_bonus = entropies.mean()\n",
    "\n",
    "# combine them\n",
    "total_loss = actor_loss + critic_loss - 0.01 * entropy_bonus\n",
    "print(\"Combined loss\")\n",
    "print(total_loss)\n",
    "\n",
    "# backpropagate\n",
    "\n",
    "optimizer.zero_grad()\n",
    "total_loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974e302d1fdb028",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER: In contrast, having a shared network will be a more efficient use of resources and samples, but because loss is combined, a desirable/undesirable result in an actor may be overshadowed by a converse result with greater scale in the critic, or vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb645eb009b85b1c",
   "metadata": {},
   "source": [
    "## Section 2: Auto-Adaptive Network Setup for Environments\n",
    "\n",
    "You will now create a function that builds a shared actor-critic network that adapts to any Gymnasium environment. This function should inspect the environment and build input/output layers accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223b6ddf43abee5",
   "metadata": {},
   "source": [
    "### Task 2: Auto-generate Input and Output Layers\n",
    "Write a function `create_shared_network(env)` that constructs a neural network using the following rules:\n",
    "- The input layer should match the environment's observation space.\n",
    "- The output layer for the **actor** should depend on the action space:\n",
    "  - For discrete actions: output probabilities using `nn.Softmax`.\n",
    "  - For continuous actions: output mean and log std for a Gaussian distribution.\n",
    "- The **critic** always outputs a single scalar value.\n",
    "\n",
    "```python\n",
    "# TODO: Define function `create_shared_network(env)`\n",
    "```\n",
    "\n",
    "#### Environments to Support:\n",
    "Test your function with the following environments:\n",
    "1. `CliffWalking-v0` (Use one-hot encoding for discrete integer observations.)\n",
    "2. `LunarLander-v3` (Standard Box space for observations and discrete actions.)\n",
    "3. `PongNoFrameskip-v4` (Use gym wrappers for Atari image preprocessing.)\n",
    "4. `HalfCheetah-v5` (Continuous observation and continuous action.)\n",
    "\n",
    "```python\n",
    "# TODO: Loop through environments and test `create_shared_network`\n",
    "```\n",
    "\n",
    "Hint: Use `gym.spaces` utilities to determine observation/action types dynamically.\n",
    "\n",
    "ðŸ”— Observation/Action Space Docs:\n",
    "- https://gymnasium.farama.org/api/spaces/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6d249ff9277403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CliffWalking-v0 -------------------\n",
      "Action probabilities distribution: \n",
      "tensor([[0.2320, 0.2751, 0.2644, 0.2285]])\n",
      "State value:\n",
      "tensor([[0.0209]])\n",
      "LunarLander-v3 -------------------\n",
      "Action probabilities distribution: \n",
      "tensor([[0.2456, 0.2397, 0.2558, 0.2589]])\n",
      "State values:\n",
      "tensor([[0.0889]])\n"
     ]
    }
   ],
   "source": [
    "class SharedActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, env_is_discrete):\n",
    "        super(SharedActorCritic, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.env_is_discrete = env_is_discrete\n",
    "\n",
    "        # base\n",
    "        self.shared_base = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # actor head\n",
    "        # The output layer for the actor should depend on the action space -----\n",
    "          # For discrete actions: output probabilities using nn.Softmax.\n",
    "        if self.env_is_discrete:\n",
    "          self.actor_head = nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, action_dim),\n",
    "                    nn.Softmax(dim=-1))\n",
    "        # For continuous actions: output mean and log std for a Gaussian distribution.\n",
    "        else:\n",
    "          self.actor_mean = nn.Linear(hidden_dim, self.action_dim)\n",
    "          self.actor_logstd = nn.Parameter(torch.zeros(self.action_dim))\n",
    "\n",
    "        # critic head\n",
    "        self.critic_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        shared_features = self.shared_base(state)\n",
    "\n",
    "        if self.env_is_discrete:\n",
    "            action_probs = self.actor_head(shared_features)\n",
    "            action_dist = torch.distributions.Categorical(action_probs)\n",
    "        else:\n",
    "            action_mean = self.actor_mean(shared_features)\n",
    "            action_std = torch.exp(self.actor_logstd)\n",
    "            action_dist = torch.distributions.Normal(action_mean, action_std)\n",
    "\n",
    "        state_value = self.critic_head(shared_features)\n",
    "        return action_dist, state_value\n",
    "\n",
    "\n",
    "def create_shared_network(env):\n",
    "    #observation space\n",
    "    obs_space = env.observation_space\n",
    "    if isinstance(obs_space, gym.spaces.Discrete):\n",
    "        state_dim = obs_space.n\n",
    "        obs_processor = lambda obs: F.one_hot(torch.tensor(obs), num_classes=state_dim).float()\n",
    "    elif isinstance(obs_space, gym.spaces.Box):\n",
    "        state_dim = obs_space.shape[0]\n",
    "        obs_processor = lambda obs: torch.tensor(obs, dtype=torch.float32)\n",
    "    else:\n",
    "        raise(\"Observation space bad\")\n",
    "\n",
    "    # action space\n",
    "    act_space = env.action_space\n",
    "    if isinstance(act_space, gym.spaces.Discrete):\n",
    "        action_dim = act_space.n\n",
    "        is_discrete_action = True\n",
    "    elif isinstance(act_space, gym.spaces.Box):\n",
    "        action_dim = act_space.shape[0]\n",
    "        is_discrete_action = False\n",
    "    else:\n",
    "        raise(\"action space bad\")\n",
    "\n",
    "    model = SharedActorCritic(state_dim, hidden_dim, action_dim, is_discrete_action)\n",
    "    return model, obs_processor\n",
    "\n",
    "def CliffWalking():\n",
    "# CliffWalking-v0 (Use one-hot encoding for discrete integer observations.)\n",
    "  env = gym.make(\"CliffWalking-v0\")\n",
    "  obs, info = env.reset()\n",
    "  state_dim = env.observation_space.n\n",
    "  hidden_dim = 128\n",
    "  action_dim = env.action_space.n\n",
    "  env_is_discrete = True\n",
    "  instance = SharedActorCritic(state_dim,hidden_dim,action_dim,env_is_discrete)\n",
    "\n",
    "  # One-hot: https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html\n",
    "  my_tensor = torch.tensor(obs, dtype=torch.long)\n",
    "  one_hot_results = F.one_hot(my_tensor, state_dim).float().unsqueeze(0)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      action_probs, state_values = instance(one_hot_results)\n",
    "  print(\"CliffWalking-v0 -------------------\")\n",
    "  print(\"Action probabilities distribution: \")\n",
    "  print(action_probs.probs)\n",
    "  print(\"State value:\")\n",
    "  print(state_values)\n",
    "\n",
    "def LunarLander():\n",
    "# LunarLander-v3 (Standard Box space for observations and discrete actions.)\n",
    "  env = gym.make(\"LunarLander-v3\")\n",
    "  obs, info = env.reset()\n",
    "  state_dim = env.observation_space.shape[0]\n",
    "  hidden_dim = 128\n",
    "  action_dim = env.action_space.n\n",
    "  env_is_discrete = True\n",
    "  instance = SharedActorCritic(state_dim,hidden_dim,action_dim,env_is_discrete)\n",
    "  my_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      action_probs, state_values = instance(my_tensor)\n",
    "\n",
    "  print(\"LunarLander-v3 -------------------\")\n",
    "  print(\"Action probabilities distribution: \")\n",
    "  print(action_probs.probs)\n",
    "  print(\"State values:\")\n",
    "  print(state_values)\n",
    "\n",
    "\n",
    "tests = [CliffWalking, LunarLander]\n",
    "for test in tests:\n",
    "  test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd13f0b62b30ff",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER: In this case they will be shared because it will be more efficient to train them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c886fa536a639",
   "metadata": {},
   "source": [
    "### Task 3: Write Observation Normalization Function\n",
    "Create a function `normalize_observation(obs, env)` that:\n",
    "- Checks if the observation space is `Box` and has `low` and `high` attributes.\n",
    "- If so, normalize the input observation.\n",
    "- Otherwise, return the observation unchanged.\n",
    "\n",
    "```python\n",
    "# TODO: Define `normalize_observation(obs, env)`\n",
    "```\n",
    "\n",
    "Test this function with observations from:\n",
    "- `LunarLander-v3`\n",
    "- `PongNoFrameskip-v4`\n",
    "\n",
    "Note: Atari observations are image arrays. Normalize pixel values to [0, 1]. For LunarLander-v3, the different elements in the observation vector have different ranges. Normalize them to [0, 1] using the `low` and `high` attributes of the observation space.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc7ee06112cf7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN_YOUR_CODE\n",
    "\n",
    "def normalize_observation(obs, env):\n",
    "\n",
    "    if env.observation_space.__class is gym.spaces.Box:\n",
    "        space = env.observation_space\n",
    "        if hasattr(space, 'low') and hasattr(space, 'high'):\n",
    "          # https://numpy.org/doc/2.2/reference/generated/numpy.where.html\n",
    "            denominator = np.where(space.high - space.low == 0, 1, space.high - space.low)\n",
    "            normalized = (obs - space.low) / denominator\n",
    "            return normalized\n",
    "    return obs\n",
    "\n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ed2a6e7ca7a7b",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER: Normalizing values helps to improve consistency in training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5fb5353307f514",
   "metadata": {},
   "source": [
    "## Section 4: Gradient Clipping\n",
    "\n",
    "To prevent exploding gradients, it's common practice to clip gradients before optimizer updates.\n",
    "\n",
    "### Task 4: Clip Gradients for Actor-Critic Networks\n",
    "Use dummy tensors and apply gradient clipping with the following PyTorch method:\n",
    "```python\n",
    "# During training, after loss.backward():\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "```\n",
    "\n",
    "Reuse the loss computation from Task 1a or 1b. After computing the gradients, apply gradient clipping.\n",
    "Print the gradient norm before and after clipping to verify itâ€™s applied.\n",
    "\n",
    "ðŸ”— PyTorch Docs: https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7327507fb6e803ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm before clipping:1.4824100810824614\n",
      "Gradient norm before clipping:0.49999966196951495\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_YOUR_CODE\n",
    "\n",
    "norm_prev = 0.0\n",
    "for i in actor_critic_instance.parameters():\n",
    "    if i.grad is not None:\n",
    "        param_norm = i.grad.data.norm(2)\n",
    "        norm_prev += param_norm.item() ** 2\n",
    "norm_prev = norm_prev ** 0.5\n",
    "print(\"Gradient norm before clipping:\" + str(norm_prev))\n",
    "\n",
    "torch.nn.utils.clip_grad_norm_(actor_critic_instance.parameters(), max_norm=0.5)\n",
    "\n",
    "norm_after = 0.0\n",
    "for j in actor_critic_instance.parameters():\n",
    "    if j.grad is not None:\n",
    "        param_norm = j.grad.data.norm(2)\n",
    "        norm_after += param_norm.item() ** 2\n",
    "norm_after = norm_after ** 0.5\n",
    "print(\"Gradient norm before clipping:\" + str(norm_after))\n",
    "\n",
    "\n",
    "# END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952750fa74cd487",
   "metadata": {},
   "source": [
    "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
    "\n",
    "YOUR ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cff31e6c6e7e4a",
   "metadata": {},
   "source": [
    "If you are working in a team, provide a contribution summary.\n",
    "| Team Member | Step# | Contribution (%) |\n",
    "|---|---|---|\n",
    "|  Naglis Paunksnis | Task 1 |  100 |\n",
    "|  Naglis Paunksnis | Task 2 |  100 |\n",
    "|  Naglis Paunksnis | Task 3 | 100  |\n",
    "|  Naglis Paunksnis | Task 4 | 100  |\n",
    "|  Naglis Paunksnis | **Total** | 100  |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
