{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d830004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from attrdict import AttrDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bcffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to PyBullet and set up the environment\n",
    "p.connect(p.GUI)\n",
    "p.resetSimulation()\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "# p.configureDebugVisualizer(p.COV_ENABLE_GUI, 0)  # Optionally hide the PyBullet GUI\n",
    "\n",
    "# Load the KUKA robot and environment objects\n",
    "planeId = p.loadURDF(\"plane.urdf\")\n",
    "cuboid_green_id = p.loadURDF(\"./object/block.urdf\", [0.54, 0, 0.02], [0, 0, 0, 1])\n",
    "kuka_id = p.loadURDF(\"kuka_iiwa/kuka_with_prismatic_gripper.urdf\")\n",
    "\n",
    "p.setGravity(0, 0, -10)\n",
    "p.resetDebugVisualizerCamera(\n",
    "    cameraDistance=2.0,    # Zoom level (2 meters away)\n",
    "    cameraYaw=75,          # Rotation around the vertical axis\n",
    "    cameraPitch=-40,       # Tilt downward\n",
    "    cameraTargetPosition=[0, 0, 0]  # Focus on the origin\n",
    ")\n",
    "\n",
    "numJoints = p.getNumJoints(kuka_id)  # Total joints: KUKA + gripper\n",
    "\n",
    "# Initialize the joints dictionary\n",
    "joints = AttrDict()\n",
    "\n",
    "# Populate the joints dictionary with information about each joint\n",
    "for joint_index in range(numJoints):\n",
    "    joint_info = p.getJointInfo(kuka_id, joint_index)\n",
    "    joint_name = joint_info[1].decode(\"utf-8\")\n",
    "    joints[joint_name] = AttrDict({\n",
    "        \"id\": joint_info[0],\n",
    "        \"lowerLimit\": joint_info[8],\n",
    "        \"upperLimit\": joint_info[9],\n",
    "        \"maxForce\": joint_info[10],\n",
    "        \"maxVelocity\": joint_info[11],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b0c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BulletKukaEnvironment(gym.Env):\n",
    "    def __init__(self, gui=True):\n",
    "\n",
    "        self.sim_time_step = 1 / 240  # TODO\n",
    "        self.eff_index = 7\n",
    "        self.home_kuka_joint_angle = np.array([0.0] * 7)\n",
    "        self.init_kuka_joint_angle = np.array([-0., 0.44, 0., -2.086, -0., 0.615, -0.])\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)  # 0: xPlus, 1: xMinus\n",
    "        self.observation_space = spaces.Box(low=np.array([0.0, -np.pi]), high=np.array([1.0, np.pi]), dtype=np.float32)\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 20\n",
    "\n",
    "    #     self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.set_kuka_joint_angles(self.home_kuka_joint_angle, self.init_kuka_joint_angle, duration=1)\n",
    "        self.operate_gripper(init_pos=0., fin_pos=0.01, duration=1)  # Open gripper\n",
    "\n",
    "        self.step_count = 0\n",
    "        eef_pos, _ = self.get_current_eef_pose()\n",
    "        obj_tilt = self.get_object_state(cuboid_green_id)[1]\n",
    "        obs = np.array([eef_pos[0], obj_tilt], dtype=np.float32)\n",
    "        return obs\n",
    "\n",
    "    #     return self.get_observation()\n",
    "\n",
    "\n",
    "    def calculate_ik(self, position, orientation):\n",
    "        quaternion = p.getQuaternionFromEuler(orientation)\n",
    "        lower_limits = [-np.pi] * 7\n",
    "        upper_limits = [np.pi] * 7\n",
    "        joint_ranges = [2 * np.pi] * 7\n",
    "        rest_poses = [(-0.0, -0.0, 0.0, -0.0, -0.0, 0.0, 0.0)]  # Rest pose for 7 DOF\n",
    "        # Calculate inverse kinematics with a 7-element damping vector\n",
    "        joint_angles = p.calculateInverseKinematics(\n",
    "            kuka_id, self.eff_index, position, quaternion,\n",
    "            jointDamping=[0.01] * 7,\n",
    "            lowerLimits=lower_limits,\n",
    "            upperLimits=upper_limits,\n",
    "            jointRanges=joint_ranges,\n",
    "            restPoses=rest_poses\n",
    "        )\n",
    "        return joint_angles\n",
    "\n",
    "    def set_kuka_joint_angles(self, init_joint_angles, des_joint_angles, duration):\n",
    "        control_joints = [\"lbr_iiwa_joint_1\", \"lbr_iiwa_joint_2\", \"lbr_iiwa_joint_3\",\n",
    "                        \"lbr_iiwa_joint_4\", \"lbr_iiwa_joint_5\", \"lbr_iiwa_joint_6\",\n",
    "                        \"lbr_iiwa_joint_7\"]\n",
    "        poses = []\n",
    "        indexes = []\n",
    "        forces = []\n",
    "        for i, name in enumerate(control_joints):\n",
    "            joint = joints[name]\n",
    "            poses.append(des_joint_angles[i])\n",
    "            indexes.append(joint.id)\n",
    "            forces.append(joint.maxForce)\n",
    "        trajectory = self.interpolate_trajectory(init_joint_angles, des_joint_angles, duration)\n",
    "        for q_t in trajectory:\n",
    "            p.setJointMotorControlArray(\n",
    "                kuka_id, indexes,\n",
    "                controlMode=p.POSITION_CONTROL,\n",
    "                targetPositions=q_t,\n",
    "                forces=forces\n",
    "            )\n",
    "            p.stepSimulation()\n",
    "            time.sleep(self.sim_time_step)\n",
    "\n",
    "    def position_path(self, t, t_max, start_pos, end_pos):\n",
    "        return start_pos + (end_pos - start_pos) * (t / t_max)\n",
    "\n",
    "    def orientation_path(self, t, t_max, start_orient, end_orient):\n",
    "        \"\"\"Orientation path (Euler angles).\"\"\"\n",
    "        return start_orient + (end_orient - start_orient) * (t / t_max)\n",
    "\n",
    "    def get_current_eef_pose(self):\n",
    "        linkstate = p.getLinkState(kuka_id, self.eff_index, computeForwardKinematics=True)\n",
    "        position, orientation = linkstate[0], linkstate[1]\n",
    "        position = list(position)\n",
    "        position[2] = position[2] - 0.0491\n",
    "        return (position, list(p.getEulerFromQuaternion(orientation)))\n",
    "\n",
    "    def get_current_joint_angles(self, kuka_or_gripper=None):\n",
    "        joint_states = p.getJointStates(kuka_id, list(range(numJoints)))\n",
    "        joint_values = [state[0] for state in joint_states]\n",
    "        if kuka_or_gripper == 'kuka':\n",
    "            return joint_values[:7]  # First 7 joints for the KUKA arm\n",
    "        elif kuka_or_gripper == 'gripper':\n",
    "            return joint_values[7:]  # Remaining joints for the gripper\n",
    "        else:\n",
    "            return joint_values\n",
    "\n",
    "    def interpolate_trajectory(self, q_start, q_end, duration):\n",
    "        num_steps = int(duration / self.sim_time_step) + 1\n",
    "        trajectory = []\n",
    "        for t in range(num_steps):\n",
    "            alpha = t / (num_steps - 1)\n",
    "            q_t = [q_start[i] + alpha * (q_end[i] - q_start[i]) for i in range(len(q_start))]\n",
    "            trajectory.append(q_t)\n",
    "        return trajectory\n",
    "\n",
    "    def moveL(self, start_pos, end_pos, target_ori, duration):\n",
    "        \"\"\"\n",
    "        Moves the robot's end-effector in a straight line from start_pos to end_pos.\n",
    "        \"\"\"\n",
    "        num_steps = int(duration / self.sim_time_step) + 1\n",
    "        target_quat = p.getQuaternionFromEuler(target_ori)\n",
    "        for step in range(num_steps):\n",
    "            alpha = step / (num_steps - 1)\n",
    "            current_pos = np.array(start_pos) + alpha * (np.array(end_pos) - np.array(start_pos))\n",
    "            # Calculate inverse kinematics for the current target position and fixed orientation\n",
    "            joint_poses = p.calculateInverseKinematics(kuka_id, self.eff_index, current_pos.tolist(), target_quat)\n",
    "            # Assuming that the arm joints are the first 7 joints\n",
    "            control_joint_indices = list(range(7))\n",
    "            target_positions = joint_poses[:7]\n",
    "            p.setJointMotorControlArray(kuka_id, control_joint_indices,\n",
    "                                        controlMode=p.POSITION_CONTROL,\n",
    "                                        targetPositions=target_positions)\n",
    "            p.stepSimulation()\n",
    "            time.sleep(self.sim_time_step)\n",
    "\n",
    "    def execute_task_space_trajectory(self, start_pos, final_pos, duration=1):\n",
    "        # final_pos is a two-element structure: [position, orientation]\n",
    "        all_joint_angles = self.calculate_ik(final_pos[0], final_pos[1])\n",
    "        des_kuka_joint_angles = all_joint_angles[:7]  # Use the first 7 joint angles\n",
    "        self.set_kuka_joint_angles(self.get_current_joint_angles(\"kuka\"), des_kuka_joint_angles, duration)\n",
    "\n",
    "    def operate_gripper(self, init_pos, fin_pos, duration=1):\n",
    "        \"\"\"\n",
    "        Smoothly open or close the gripper by interpolating the gripper opening angle.\n",
    "        Args:\n",
    "            duration (float): Duration for the gripper motion (seconds).\n",
    "            init_pos (float): Initial gripper opening distance.\n",
    "            fin_pos (float): Final gripper opening distance.\n",
    "        \"\"\"\n",
    "        control_joints = [\"left_finger_sliding_joint\", \"right_finger_sliding_joint\"]\n",
    "        poses = []\n",
    "        indexes = []\n",
    "        forces = []\n",
    "        \n",
    "        init_arr = np.array([-init_pos, init_pos])\n",
    "        fin_arr = np.array([-fin_pos, fin_pos])\n",
    "        for i, name in enumerate(control_joints):\n",
    "            joint = joints[name]\n",
    "            poses.append(fin_arr[i])\n",
    "            indexes.append(joint.id)\n",
    "            forces.append(joint.maxForce)\n",
    "        trajectory = self.interpolate_trajectory(init_arr, fin_arr, duration)\n",
    "        for q_t in trajectory:\n",
    "            p.setJointMotorControlArray(\n",
    "                kuka_id, indexes,\n",
    "                controlMode=p.POSITION_CONTROL,\n",
    "                targetPositions=q_t,\n",
    "                forces=forces\n",
    "            )\n",
    "            p.stepSimulation()\n",
    "            time.sleep(self.sim_time_step)\n",
    "\n",
    "    def get_object_state(self, object_id):\n",
    "        position, orientation = p.getBasePositionAndOrientation(object_id)\n",
    "        orientation_euler = p.getEulerFromQuaternion(orientation)\n",
    "        return orientation_euler \n",
    "        \n",
    "\n",
    "    def execute_grip_action(self):\n",
    "        \"\"\" \n",
    "        Move the gripper down, grip the object, lift it up and put it down again, \n",
    "        finally bring the gripper up to its initial position.\n",
    "        \"\"\"\n",
    "\n",
    "        current_eef_pos, current_eef_orien = self.get_current_eef_pose()\n",
    "        next_eef_pos = current_eef_pos.copy()\n",
    "        next_eef_pos[2] = next_eef_pos[2] - 0.1225\n",
    "\n",
    "        self.moveL(current_eef_pos, next_eef_pos, current_eef_orien, duration=1)\n",
    "        self.operate_gripper(init_pos=0.01, fin_pos=0.0008, duration=0.5)  # Close gripper\n",
    "        self.moveL(next_eef_pos, current_eef_pos, current_eef_orien, duration=1)\n",
    "        obj_tilt_angle = self.get_object_state(cuboid_green_id)[1]\n",
    "        self.moveL(current_eef_pos, next_eef_pos, current_eef_orien, duration=1)\n",
    "        self.operate_gripper(init_pos=0., fin_pos=0.01, duration=1)  # Open gripper\n",
    "        self.moveL(next_eef_pos, current_eef_pos, current_eef_orien, duration=1)\n",
    "\n",
    "        return obj_tilt_angle\n",
    "\n",
    "    # def execute_grip_action(self):\n",
    "    #     \"\"\" \n",
    "    #     Move the gripper down, grip the object, lift it up and put it down again, \n",
    "    #     finally bring the gripper up to its initial position. Also detects if the object was grasped.\n",
    "    #     \"\"\"\n",
    "    #     current_eef_pos, current_eef_orien = self.get_current_eef_pose()\n",
    "    #     grasp_pos = current_eef_pos.copy()\n",
    "    #     grasp_pos[2] -= 0.1225  \n",
    "\n",
    "    #     self.moveL(current_eef_pos, grasp_pos, current_eef_orien, duration=1)\n",
    "    #     self.operate_gripper(init_pos=0.01, fin_pos=0.0008, duration=0.5)\n",
    "\n",
    "    #     before_lift_pos, _ = p.getBasePositionAndOrientation(cuboid_green_id)\n",
    "    #     self.moveL(grasp_pos, current_eef_pos, current_eef_orien, duration=1)\n",
    "\n",
    "    #     after_lift_pos, _ = p.getBasePositionAndOrientation(cuboid_green_id)\n",
    "\n",
    "    #     lifted = (after_lift_pos[2] - before_lift_pos[2]) > 0.015\n",
    "\n",
    "    #     obj_tilt_angle = self.get_object_state(cuboid_green_id)[1]\n",
    "\n",
    "    #     self.moveL(current_eef_pos, grasp_pos, current_eef_orien, duration=1)\n",
    "    #     self.operate_gripper(init_pos=0., fin_pos=0.01, duration=1)  # Open gripper\n",
    "    #     self.moveL(grasp_pos, current_eef_pos, current_eef_orien, duration=1)\n",
    "\n",
    "    #     return obj_tilt_angle, lifted\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "\n",
    "        x_step = 0.05\n",
    "        current_eef_pos, current_eef_orien = self.get_current_eef_pose()\n",
    "        next_eef_pos = current_eef_pos.copy()\n",
    "\n",
    "        if action == 0:  # xPlus\n",
    "            next_eef_pos[0] += x_step\n",
    "        elif action == 1:  # xMinus\n",
    "            next_eef_pos[0] -= x_step\n",
    "\n",
    "        self.moveL(current_eef_pos, next_eef_pos, current_eef_orien, duration=2)\n",
    "\n",
    "        # tilt, lifted = self.execute_grip_action()\n",
    "        tilt = self.execute_grip_action()\n",
    "\n",
    "        obj_pos, _ = p.getBasePositionAndOrientation(cuboid_green_id)\n",
    "        obs = np.array([next_eef_pos[0], tilt], dtype=np.float32)\n",
    "\n",
    "        tilt_penalty = abs(tilt) * 5\n",
    "        height_reward = max(0, obj_pos[2] - 0.03) * 10  \n",
    "        position_reward = -abs(next_eef_pos[0] - 0.54) * 2 \n",
    "        # grasp_bonus = 5.0 if lifted else 0.0\n",
    "\n",
    "        # reward = height_reward - tilt_penalty + position_reward + grasp_bonus\n",
    "        reward = height_reward - tilt_penalty + position_reward\n",
    "\n",
    "        terminated = (self.step_count >= self.max_steps)\n",
    "\n",
    "        # Debug info\n",
    "        info = {\n",
    "            \"tilt_angle\": tilt,\n",
    "            \"obj_height\": obj_pos[2],\n",
    "            # \"lifted\": lifted\n",
    "        }\n",
    "\n",
    "        return obs, reward, terminated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=1000):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha \n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame = 1 \n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
    "        self.position = 0\n",
    "    \n",
    "    def beta_by_frame(self, frame_idx):\n",
    " \n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.position] = experience\n",
    "        \n",
    "        self.priorities[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:len(self.buffer)]\n",
    "        \n",
    "        priorities = np.maximum(priorities, 1e-5)\n",
    "        \n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        \n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame += 1\n",
    "        \n",
    "        weights = (len(self.buffer) * probabilities[indices]) ** (-beta)\n",
    "        weights /= weights.max() \n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(states))\n",
    "        next_states = torch.FloatTensor(np.array(next_states))\n",
    "        actions = torch.LongTensor(np.array(actions)).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(np.array(rewards)).unsqueeze(1)\n",
    "        dones = torch.FloatTensor(np.array(dones)).unsqueeze(1)\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(1)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class PERDQN:\n",
    "    def __init__(self, env, gamma=0.99, batch_size=16):\n",
    "        self.env = env\n",
    "        self.state_dim = 2  \n",
    "        self.action_dim = 2  \n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = 10  \n",
    "        self.learning_rate = 1e-3 \n",
    "        \n",
    "        self.Q_policy = NeuralNetwork(self.state_dim, self.action_dim)\n",
    "        self.Q_prime = NeuralNetwork(self.state_dim, self.action_dim)\n",
    "        self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "        self.Q_prime.eval()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.Q_policy.parameters(), lr=self.learning_rate)\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(capacity=2000) \n",
    "        \n",
    "        self.total_steps = 0\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.9  \n",
    "        self.rewards_history = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.Q_policy(state)\n",
    "                return q_values.max(1)[1].item()\n",
    "\n",
    "    def update_model(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones, indices, weights = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        q_values = self.Q_policy(states).gather(1, actions)\n",
    "\n",
    "        # next_actions = self.Q_policy(next_states).max(1)[1].unsqueeze(1)\n",
    "        # next_q_values = self.Q_prime(next_states).gather(1, next_actions)\n",
    "\n",
    "        next_q_values = self.Q_prime(next_states).max(1)[0].unsqueeze(1)\n",
    "\n",
    "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        td_errors = torch.abs(q_values - expected_q_values).detach().cpu().numpy()\n",
    "        loss = (weights * F.smooth_l1_loss(q_values, expected_q_values, reduction='none')).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.Q_policy.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.replay_buffer.update_priorities(indices, td_errors + 1e-5)\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, num_episodes=500):\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            actions_taken = []\n",
    "\n",
    "            for step in range(self.env.max_steps):\n",
    "                action = self.select_action(state)\n",
    "                actions_taken.append(action)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "                self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "                if len(self.replay_buffer) >= self.batch_size:\n",
    "                    self.update_model()\n",
    "\n",
    "                if self.total_steps % self.target_update_freq == 0:\n",
    "                    self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "                state = next_state\n",
    "                self.total_steps += 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            self.rewards_history.append(episode_reward)\n",
    "\n",
    "            avg_reward = np.mean(self.rewards_history[-10:]) if len(self.rewards_history) >= 10 else np.mean(self.rewards_history)\n",
    "            print(f\"Episode {episode+1}/{num_episodes} | Reward: {episode_reward:.2f} | Avg Reward: {avg_reward:.2f} | Epsilon: {self.epsilon:.4f}\")\n",
    "            print(f\"Actions taken: {actions_taken}\")\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "        return self.rewards_history\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BulletKukaEnvironment()\n",
    "agent = PERDQN(env)\n",
    "rewards = agent.train(num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e7187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
