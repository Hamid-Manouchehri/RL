{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d9fbe8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reinforcement Learning Final Project\n",
    "\n",
    "**File:** `test_model.ipynb`  \n",
    "**Author:** Hamid Manouchehri & Christ Joe Maria Anantharaj  \n",
    "**Email:** hmanouch@buffalo.edu & christjo@buffalo.edu  \n",
    "**Date:** May 4, 2025\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Description:\n",
    "Reading the \".pth\" file and running that in Pybullet sim\n",
    "\n",
    "---\n",
    "\n",
    "### License:\n",
    "This script is licensed under the **MIT License**.  \n",
    "You may obtain a copy of the License at:  \n",
    "ðŸ”— [https://opensource.org/licenses/MIT](https://opensource.org/licenses/MIT)\n",
    "\n",
    "**SPDX-License-Identifier:** MIT\n",
    "\n",
    "---\n",
    "\n",
    "### Disclaimer:\n",
    "> This software is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and noninfringement.  \n",
    "> In no event shall the authors be liable for any claim, damages, or other liability, whether in an action of contract, tort, or otherwise, arising from, out of, or in connection with the software or the use or other dealings in the software.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from attrdict import AttrDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f4a3d",
   "metadata": {},
   "source": [
    "### Intitial loading of the pybullet environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba510d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to PyBullet and set up the environment\n",
    "p.connect(p.GUI)\n",
    "# p.connect(p.DIRECT)\n",
    "p.resetSimulation()\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "# p.configureDebugVisualizer(p.COV_ENABLE_GUI, 0)  # Optionally hide the PyBullet GUI\n",
    "\n",
    "# Load the KUKA robot and environment objects\n",
    "planeId = p.loadURDF(\"plane.urdf\")\n",
    "cuboid_green_id = p.loadURDF(\"./object/block.urdf\", [0.54, 0, 0.02], [0, 0, 0, 1])\n",
    "kuka_id = p.loadURDF(\"kuka_iiwa/kuka_with_prismatic_gripper.urdf\")\n",
    "\n",
    "p.setGravity(0, 0, -10)\n",
    "p.resetDebugVisualizerCamera(\n",
    "    cameraDistance=2.0,    # Zoom level (2 meters away)\n",
    "    cameraYaw=75,          # Rotation around the vertical axis\n",
    "    cameraPitch=-40,       # Tilt downward\n",
    "    cameraTargetPosition=[0, 0, 0]  # Focus on the origin\n",
    ")\n",
    "\n",
    "numJoints = p.getNumJoints(kuka_id)  # Total joints: KUKA + gripper\n",
    "\n",
    "# Initialize the joints dictionary\n",
    "joints = AttrDict()\n",
    "\n",
    "# Populate the joints dictionary with information about each joint\n",
    "for joint_index in range(numJoints):\n",
    "    joint_info = p.getJointInfo(kuka_id, joint_index)\n",
    "    joint_name = joint_info[1].decode(\"utf-8\")\n",
    "    joints[joint_name] = AttrDict({\n",
    "        \"id\": joint_info[0],\n",
    "        \"lowerLimit\": joint_info[8],\n",
    "        \"upperLimit\": joint_info[9],\n",
    "        \"maxForce\": joint_info[10],\n",
    "        \"maxVelocity\": joint_info[11],\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759f479",
   "metadata": {},
   "source": [
    "### Defining the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc91b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BulletKukaEnvironment(gym.Env):\n",
    "    def __init__(self, gui=True):\n",
    "\n",
    "        self.sim_time_step = 1 / 240  # TODO\n",
    "        self.eff_index = 7\n",
    "        self.home_kuka_joint_angle = np.array([0.0] * 7)\n",
    "        self.init_kuka_joint_angle = np.array([-0., 0.44, 0., -2.086, -0., 0.615, -0.])\n",
    "\n",
    "    #     self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.set_kuka_joint_angles(self.home_kuka_joint_angle, self.init_kuka_joint_angle, duration=1)\n",
    "        self.operate_gripper(init_pos=0., fin_pos=0.01, duration=1)  # Open gripper\n",
    "\n",
    "        return self.get_observation()\n",
    "\n",
    "\n",
    "    def get_observation(self):\n",
    "        \n",
    "        pass\n",
    "        # return 0\n",
    "\n",
    "\n",
    "    def calculate_ik(self, position, orientation):\n",
    "        quaternion = p.getQuaternionFromEuler(orientation)\n",
    "        lower_limits = [-np.pi] * 7\n",
    "        upper_limits = [np.pi] * 7\n",
    "        joint_ranges = [2 * np.pi] * 7\n",
    "        rest_poses = [(-0.0, -0.0, 0.0, -0.0, -0.0, 0.0, 0.0)]  # Rest pose for 7 DOF\n",
    "        # Calculate inverse kinematics with a 7-element damping vector\n",
    "        joint_angles = p.calculateInverseKinematics(\n",
    "            kuka_id, self.eff_index, position, quaternion,\n",
    "            jointDamping=[0.01] * 7,\n",
    "            lowerLimits=lower_limits,\n",
    "            upperLimits=upper_limits,\n",
    "            jointRanges=joint_ranges,\n",
    "            restPoses=rest_poses\n",
    "        )\n",
    "        return joint_angles\n",
    "\n",
    "    def set_kuka_joint_angles(self, init_joint_angles, des_joint_angles, duration):\n",
    "        control_joints = [\"lbr_iiwa_joint_1\", \"lbr_iiwa_joint_2\", \"lbr_iiwa_joint_3\",\n",
    "                        \"lbr_iiwa_joint_4\", \"lbr_iiwa_joint_5\", \"lbr_iiwa_joint_6\",\n",
    "                        \"lbr_iiwa_joint_7\"]\n",
    "        poses = []\n",
    "        indexes = []\n",
    "        forces = []\n",
    "        for i, name in enumerate(control_joints):\n",
    "            joint = joints[name]\n",
    "            poses.append(des_joint_angles[i])\n",
    "            indexes.append(joint.id)\n",
    "            forces.append(joint.maxForce)\n",
    "        trajectory = self.interpolate_trajectory(init_joint_angles, des_joint_angles, duration)\n",
    "        for q_t in trajectory:\n",
    "            p.setJointMotorControlArray(\n",
    "                kuka_id, indexes,\n",
    "                controlMode=p.POSITION_CONTROL,\n",
    "                targetPositions=q_t,\n",
    "                forces=forces\n",
    "            )\n",
    "            p.stepSimulation()\n",
    "            time.sleep(self.sim_time_step)\n",
    "\n",
    "    def position_path(self, t, t_max, start_pos, end_pos):\n",
    "        return start_pos + (end_pos - start_pos) * (t / t_max)\n",
    "\n",
    "    def orientation_path(self, t, t_max, start_orient, end_orient):\n",
    "        \"\"\"Orientation path (Euler angles).\"\"\"\n",
    "        return start_orient + (end_orient - start_orient) * (t / t_max)\n",
    "\n",
    "    def get_current_eef_pose(self):\n",
    "        linkstate = p.getLinkState(kuka_id, self.eff_index, computeForwardKinematics=True)\n",
    "        position, orientation = linkstate[0], linkstate[1]\n",
    "        position = list(position)\n",
    "        position[2] = position[2] - 0.0491\n",
    "        return (position, list(p.getEulerFromQuaternion(orientation)))\n",
    "\n",
    "    def get_current_joint_angles(self, kuka_or_gripper=None):\n",
    "        joint_states = p.getJointStates(kuka_id, list(range(numJoints)))\n",
    "        joint_values = [state[0] for state in joint_states]\n",
    "        if kuka_or_gripper == 'kuka':\n",
    "            return joint_values[:7]  # First 7 joints for the KUKA arm\n",
    "        elif kuka_or_gripper == 'gripper':\n",
    "            return joint_values[7:]  # Remaining joints for the gripper\n",
    "        else:\n",
    "            return joint_values\n",
    "\n",
    "    def interpolate_trajectory(self, q_start, q_end, duration):\n",
    "        num_steps = int(duration / self.sim_time_step) + 1\n",
    "        trajectory = []\n",
    "        for t in range(num_steps):\n",
    "            alpha = t / (num_steps - 1)\n",
    "            q_t = [q_start[i] + alpha * (q_end[i] - q_start[i]) for i in range(len(q_start))]\n",
    "            trajectory.append(q_t)\n",
    "        return trajectory\n",
    "\n",
    "    def moveL(self, start_pos, end_pos, target_ori, duration):\n",
    "        \"\"\"\n",
    "        Moves the robot's end-effector in a straight line from start_pos to end_pos.\n",
    "        \"\"\"\n",
    "        num_steps = int(duration / self.sim_time_step) + 1\n",
    "        target_quat = p.getQuaternionFromEuler(target_ori)\n",
    "        for step in range(num_steps):\n",
    "            alpha = step / (num_steps - 1)\n",
    "            current_pos = np.array(start_pos) + alpha * (np.array(end_pos) - np.array(start_pos))\n",
    "            # Calculate inverse kinematics for the current target position and fixed orientation\n",
    "            joint_poses = p.calculateInverseKinematics(kuka_id, self.eff_index, current_pos.tolist(), target_quat)\n",
    "            # Assuming that the arm joints are the first 7 joints\n",
    "            control_joint_indices = list(range(7))\n",
    "            target_positions = joint_poses[:7]\n",
    "            p.setJointMotorControlArray(kuka_id, control_joint_indices,\n",
    "                                        controlMode=p.POSITION_CONTROL,\n",
    "                                        targetPositions=target_positions)\n",
    "            p.stepSimulation()\n",
    "            time.sleep(self.sim_time_step)\n",
    "\n",
    "    def execute_task_space_trajectory(self, start_pos, final_pos, duration=1):\n",
    "        # final_pos is a two-element structure: [position, orientation]\n",
    "        all_joint_angles = self.calculate_ik(final_pos[0], final_pos[1])\n",
    "        des_kuka_joint_angles = all_joint_angles[:7]  # Use the first 7 joint angles\n",
    "        self.set_kuka_joint_angles(self.get_current_joint_angles(\"kuka\"), des_kuka_joint_angles, duration)\n",
    "\n",
    "    def operate_gripper(self, init_pos, fin_pos, duration=1):\n",
    "        \"\"\"\n",
    "        Smoothly open or close the gripper by interpolating the gripper opening angle.\n",
    "        Args:\n",
    "            duration (float): Duration for the gripper motion (seconds).\n",
    "            init_pos (float): Initial gripper opening distance.\n",
    "            fin_pos (float): Final gripper opening distance.\n",
    "        \"\"\"\n",
    "        control_joints = [\"left_finger_sliding_joint\", \"right_finger_sliding_joint\"]\n",
    "        poses = []\n",
    "        indexes = []\n",
    "        forces = []\n",
    "        \n",
    "        init_arr = np.array([-init_pos, init_pos])\n",
    "        fin_arr = np.array([-fin_pos, fin_pos])\n",
    "        for i, name in enumerate(control_joints):\n",
    "            joint = joints[name]\n",
    "            poses.append(fin_arr[i])\n",
    "            indexes.append(joint.id)\n",
    "            forces.append(joint.maxForce)\n",
    "        trajectory = self.interpolate_trajectory(init_arr, fin_arr, duration)\n",
    "        for q_t in trajectory:\n",
    "            p.setJointMotorControlArray(\n",
    "                kuka_id, indexes,\n",
    "                controlMode=p.POSITION_CONTROL,\n",
    "                targetPositions=q_t,\n",
    "                forces=forces\n",
    "            )\n",
    "            p.stepSimulation()\n",
    "            time.sleep(self.sim_time_step)\n",
    "\n",
    "    def get_object_state(self, object_id):\n",
    "        position, orientation = p.getBasePositionAndOrientation(object_id)\n",
    "        orientation_euler = p.getEulerFromQuaternion(orientation)\n",
    "        return orientation_euler \n",
    "        \n",
    "\n",
    "    def execute_grip_action(self):\n",
    "        \"\"\" \n",
    "        Move the gripper down, grip the object, lift it up and put it down again, \n",
    "        finally bring the gripper up to its initial position.\n",
    "        \"\"\"\n",
    "\n",
    "        current_eef_pos, current_eef_orien = self.get_current_eef_pose()\n",
    "        next_eef_pos = current_eef_pos.copy()\n",
    "        next_eef_pos[2] = next_eef_pos[2] - 0.1225\n",
    "\n",
    "        self.moveL(current_eef_pos, next_eef_pos, current_eef_orien, duration=1)\n",
    "        self.operate_gripper(init_pos=0.01, fin_pos=0.0008, duration=0.5)  # Close gripper\n",
    "        self.moveL(next_eef_pos, current_eef_pos, current_eef_orien, duration=1)\n",
    "        obj_tilt_angle = self.get_object_state(cuboid_green_id)[1]\n",
    "        self.moveL(current_eef_pos, next_eef_pos, current_eef_orien, duration=1)\n",
    "        self.operate_gripper(init_pos=0., fin_pos=0.01, duration=.5)  # Open gripper\n",
    "        self.moveL(next_eef_pos, current_eef_pos, current_eef_orien, duration=1)\n",
    "\n",
    "        return obj_tilt_angle\n",
    "    \n",
    "\n",
    "    def step(self, action, x_step_size=0.01):\n",
    "\n",
    "        current_eef_pos, current_eef_orien = self.get_current_eef_pose()\n",
    "        next_eef_pos = current_eef_pos.copy()\n",
    "        \n",
    "        if action == \"xPlus\":\n",
    "            next_eef_pos[0] = current_eef_pos[0] + x_step_size\n",
    "            self.moveL(current_eef_pos, next_eef_pos, current_eef_orien, duration=.5)\n",
    "            obj_tilt_angle = self.execute_grip_action()\n",
    "\n",
    "        elif action == \"xMinus\":\n",
    "            next_eef_pos[0] = current_eef_pos[0] - x_step_size\n",
    "            self.moveL(current_eef_pos, next_eef_pos, current_eef_orien, duration=.5)\n",
    "            obj_tilt_angle = self.execute_grip_action()\n",
    "\n",
    "\n",
    "        return obj_tilt_angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48011608",
   "metadata": {},
   "source": [
    "### DQN Algorithm and Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, env, gamma=0.99, batch_size=16):\n",
    "        self.env = env\n",
    "        self.state_dim = 2  \n",
    "        self.action_dim = 2  \n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.C = 10  \n",
    "        self.learning_rate = 1e-3 \n",
    "        \n",
    "        self.Q_policy = NeuralNetwork(self.state_dim, self.action_dim)\n",
    "        self.Q_prime = NeuralNetwork(self.state_dim, self.action_dim)\n",
    "        self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "        self.Q_prime.eval()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.Q_policy.parameters(), lr=self.learning_rate)\n",
    "        self.replay_buffer = deque([], maxlen=1000)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        self.total_steps = 0\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = np.exp(np.log(self.epsilon_min / 1.0) / 100)\n",
    "        self.total_rewards = []\n",
    "        self.epsilon_values = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.Q_policy(state)\n",
    "                return q_values.max(1)[1].item()\n",
    "\n",
    "    def update_model(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        \n",
    "        states = torch.tensor([s[0] for s in batch], dtype=torch.float32)\n",
    "        actions = torch.tensor([s[1] for s in batch], dtype=torch.long).unsqueeze(1)\n",
    "        rewards = torch.tensor([s[2] for s in batch], dtype=torch.float32)\n",
    "        next_states = torch.tensor([s[3] for s in batch], dtype=torch.float32)\n",
    "        dones = torch.tensor([s[4] for s in batch], dtype=torch.float32)\n",
    "\n",
    "        q_values = self.Q_policy(states).gather(1, actions)\n",
    "\n",
    "        next_q_values = self.Q_prime(next_states).max(1)[0].unsqueeze(1)\n",
    "\n",
    "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        td_errors = torch.abs(q_values - expected_q_values).detach().cpu().numpy()\n",
    "        loss = self.criterion(q_values, expected_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.Q_policy.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, num_episodes=500):\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "\n",
    "            for step in range(self.env.max_steps):\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "                self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "                if len(self.replay_buffer) >= self.batch_size:\n",
    "                    self.update_model()\n",
    "\n",
    "                if self.total_steps % self.C == 0:\n",
    "                    self.Q_prime.load_state_dict(self.Q_policy.state_dict())\n",
    "\n",
    "                state = next_state\n",
    "                self.total_steps += 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            self.epsilon_values.append(self.epsilon)\n",
    "            self.total_rewards.append(episode_reward)\n",
    "\n",
    "            print(f\"Episode {episode+1}/{num_episodes} | Reward: {episode_reward:.2f}\")\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "        return self.total_rewards\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a769269",
   "metadata": {},
   "source": [
    "### Initialize environment and rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df068b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BulletKukaEnvironment()\n",
    "terminated, truncated = False, False\n",
    "env.reset()\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd1d467",
   "metadata": {},
   "source": [
    "### Test The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ddfd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = BulletKukaEnvironment(gui=True)   # make sure this spins up the PyBullet window\n",
    "# obs = env.reset()\n",
    "\n",
    "# # state_dim  = env.observation_space.shape[0]\n",
    "# state_dim  = 2\n",
    "# # action_dim = env.action_space.n\n",
    "# action_dim = 2\n",
    "# model = NeuralNetwork(state_dim, action_dim)  # Recreating the network architecture\n",
    "# model.load_state_dict(torch.load(\"test.pth\", map_location=\"cpu\"))\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "# obs = env.reset()\n",
    "# print(obs)\n",
    "# done = False\n",
    "# total_reward = 0\n",
    "\n",
    "# # a) turn obs into a tensor\n",
    "# state_t = torch.from_numpy(obs).float().unsqueeze(0)\n",
    "# # b) forward pass\n",
    "# with torch.no_grad():\n",
    "#     q_vals = model(state_t)\n",
    "# # c) pick action (greedy for DQN, argmax; or sample from pi_theta for policy nets)\n",
    "# action = q_vals.argmax(dim=1).item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3) Run episodes and act greedily (or sample if itâ€™s a stochastic policy)\n",
    "# n_episodes = 5\n",
    "# for ep in range(n_episodes):\n",
    "#     obs = env.reset()\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "#     while not done:\n",
    "#         # a) turn obs into a tensor\n",
    "#         state_t = torch.from_numpy(obs).float().unsqueeze(0)\n",
    "#         # b) forward pass\n",
    "#         with torch.no_grad():\n",
    "#             q_vals = model(state_t)\n",
    "#         # c) pick action (greedy for DQN, argmax; or sample from pi_theta for policy nets)\n",
    "#         action = q_vals.argmax(dim=1).item()\n",
    "#         # d) step the sim\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#         total_reward += reward\n",
    "#         # e) advance the physics if you need manual stepping\n",
    "#         #    p.stepSimulation(); time.sleep(env.sim_time_step)\n",
    "#     print(f\"Episode {ep} â†’ total reward = {total_reward:.2f}\")\n",
    "\n",
    "\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466633f3",
   "metadata": {},
   "source": [
    "### Execute a single action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_step_size = 0.03  # TODO\n",
    "\n",
    "print(env.step(\"xPlus\", x_step_size))\n",
    "print(env.step(\"xPlus\", x_step_size))\n",
    "print(env.step(\"xPlus\", x_step_size))\n",
    "print(env.step(\"xPlus\", x_step_size))\n",
    "print(env.step(\"xPlus\", x_step_size))\n",
    "print(env.step(\"xPlus\", x_step_size))\n",
    "print(env.step(\"xPlus\", x_step_size))\n",
    "\n",
    "# env.render()\n",
    "\n",
    "while True:\n",
    "    p.stepSimulation()\n",
    "    time.sleep(env.sim_time_step)\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
